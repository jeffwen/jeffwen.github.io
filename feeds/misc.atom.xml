<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Jeff Wen - misc</title><link href="http://jeffwen.github.io/" rel="alternate"></link><link href="http://jeffwen.github.io/feeds/misc.atom.xml" rel="self"></link><id>http://jeffwen.github.io/</id><updated>2016-02-10T16:27:00-05:00</updated><entry><title>Movie Scraping and Regression Analysis</title><link href="http://jeffwen.github.io/2016/02/10/movie_scraping_and_regression_analysis" rel="alternate"></link><published>2016-02-10T16:27:00-05:00</published><updated>2016-02-10T16:27:00-05:00</updated><author><name>Jeff Wen</name></author><id>tag:jeffwen.github.io,2016-02-10:/2016/02/10/movie_scraping_and_regression_analysis</id><summary type="html">&lt;p&gt;Scraping movies and building regression models to predict total adjusted domestic gross&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;February 10, 2016&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In ths post, I will share a bit about what I have been up to over the past few weeks. I plan on putting up my code soon!&lt;/p&gt;
&lt;h1&gt;Getting Started&lt;/h1&gt;
&lt;p&gt;For this project, I wanted to make use of a couple things that I learned at Metis. More specifically, I was interested in using BeautifulSoup, Statsmodels, and Scikit-learn to try and predict total adjusted domestic gross from movie data scraped from &lt;a href="http://www.boxofficemojo.com"&gt;boxofficemojo&lt;/a&gt;. One of the main objectives of the project was to try and get a sense for how to collect data and make sure that it is in a clean enough format to run analyses on.&lt;/p&gt;
&lt;p&gt;One of the things that I realized while participating in kaggle competitions is that a lot of the time the data comes in very nice formats. Most of the major data errors are already taken care of and there isn't much if any scraping that needs to be done to get the data (I can just download the data). Therefore, I figured it would be nice to get a sense for how to web scrape then clean the data myself to understand the process more completely.&lt;/p&gt;
&lt;h1&gt;Objectives&lt;/h1&gt;
&lt;p&gt;Below are some of the objectives that I had in mind before starting the project:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Get familiar with using BeautifulSoup to scrape web data&lt;ul&gt;
&lt;li&gt;Put HTML knowledge to use so that I can effectively isolate and extract the information I want from boxofficemojo&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Compare and contrast using Statsmodels and Scikit-learn to do linear regression&lt;ul&gt;
&lt;li&gt;Understand how and when to use transformations to variables in order to create a better model&lt;/li&gt;
&lt;li&gt;Experiment between Lasso, Ridge, and Elastic Net to regularize the models that are built&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Learn more about utilizing cross validation as a tool to identify the best model&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;Scraping the Data&lt;/h1&gt;
&lt;p&gt;The data scraping was one of the most time consuming parts of this project because it took quite a bit of time to define the business problem enough to come up with an idea of what I needed and it was also difficult at times to navigate through the nested tables (the HTML). However, BeautifulSoup made it quite simple to grab the HTML and parse through it.&lt;/p&gt;
&lt;p&gt;More about the business problem. In order to make the project more interesting our instructors at Metis had us come up with a problem that we wanted to solve. For me, I figured I could use this the data and the model that I created to predict the total adjusted domestic gross. The motivating question for this is as a hypothetical merchandise production company (Nexus), we had already purchased rights from a couple movie studios to produce products for their movies. However, because production lines are relatively inflexible and take a while to reorganize, the business question was whether we could use a regression model to identify movies that would be more popular so that we could reorganize our production lines before crunch time (when the movies got immensely popular) to ensure that we properly allocated our resources. &lt;/p&gt;
&lt;p&gt;With this problem in mind, it was time to start the scraping process. I decided to start by scraping as many movies as possible so that I could build out my training data set. When I looked at the a &lt;a href="http://www.boxofficemojo.com/movies/?page=main&amp;amp;adjust_mo=&amp;amp;adjust_yr=2016&amp;amp;id=avatar.htm"&gt;sample page&lt;/a&gt; on boxofficemojo, I noticed that a lot of interesting information was captured in the table at the top of each movie page. As a first step, I realized it would be quick to just take down this information.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Avatar Sample" src="/images/avatar_sample.png"&gt;&lt;/p&gt;
&lt;p&gt;I wrote a couple functions that used BeautifulSoup to output the HTML, then parse the HTML to extract just the information in that table. Afterwards, I stored the information in a dictionary of dictionaries (key = movie slug; values = information in the table) then I put that information into a pandas dataframe. The dict within a dict allowed me to add details to the dictionary for each movie if I decided to get more information (which I did).&lt;/p&gt;
&lt;p&gt;I realized that if I really wanted to create a suitable training set then I needed to scrape many movies. In order to do this, I went to the alphabetical index and scraped the links of each movie (~16k) and passed each link into the previous information getting function. After these two steps, I had most of the information that I needed.&lt;/p&gt;
&lt;p&gt;The last bit of scraping I did was to get the daily sales information and the daily theater count information. Originally I figured it would be interesting to do time series analysis on the daily sales information to predict a movie's sales and while I did try an ARIMA model I decided I would go back to that later.&lt;/p&gt;
&lt;h1&gt;Data Cleaning&lt;/h1&gt;
&lt;p&gt;The next focus was to clean the data and start thinking about how to get the model up and running. I started by importing the dataframes that I had created into my Python environment.&lt;/p&gt;
&lt;p&gt;Then I  worked with the variables to make sure that they were able to be processed&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Turning the date strings into datetime objects&lt;/li&gt;
&lt;li&gt;Turning the runtime string from "1 hr. 52 min." to integer format&lt;/li&gt;
&lt;li&gt;Getting the Month, Year, and Day for each datetime object&lt;/li&gt;
&lt;li&gt;Drop the NA values&lt;/li&gt;
&lt;li&gt;Reorder the columns&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With the initial data cleaning steps completed, I moved forward to building the linear regression models.&lt;/p&gt;
&lt;h2&gt;Model Building&lt;/h2&gt;
&lt;p&gt;I wanted to create an initial plot to take a look at some of the data and make sure that things looked normal. I also created a residual plot just to check and see if the residuals were more or less random.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;From the initial plot it looks like there are quite a few values that are sitting right at 0, so I want to look and see what those are/ get rid of the ones that are NAs (though we should have already dropped these)&lt;ul&gt;
&lt;li&gt;When I looked further into this problem, it seemed like the problem was not from NAs but actually from opening sales that were exteremly small&lt;/li&gt;
&lt;li&gt;Further investigation showed that the very small observations were from releases at very few theaters vs. many theaters&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The residuals plot also looks a bit bunched together and definitely looks like there is some heteroscedasticity&lt;/li&gt;
&lt;li&gt;I decided that it might be good to transform the data to see if it could deal with some of the effect&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Initial Residuals" src="/images/initial_resids.png"&gt;&lt;/p&gt;
&lt;p&gt;After fitting the initial model, the adjusted R-squared was .457, which was not too horrible for a first go. I continued the process of analyzing the shape of the residuals and further transformed variables that seemed to have non-normal residuals. &lt;/p&gt;
&lt;p&gt;&lt;img alt="Inital Regression Results" src="/images/initial_reg.png"&gt;&lt;/p&gt;
&lt;h3&gt;Bring in more data&lt;/h3&gt;
&lt;p&gt;In order to deal with the observations that looked to bunched near 0, I brought in the number of theaters that the movie was released at to calculate the average opening sales per number of theaters.&lt;/p&gt;
&lt;h3&gt;Continuing the model building&lt;/h3&gt;
&lt;p&gt;With the new data, I continued to iterate and build models successively by tuning the parameters (mainly by plotting the residuals and looking to see if the residuals were random/ checking to see that there weren't major collinearity problems). By this time the model's adjusted R-squared was at .657.&lt;/p&gt;
&lt;p&gt;I figured it might be good to group the genres because some of the genres were occurring realtively infrequently. So I wrote a function to standardize the number of genres. However, when I ran the regression again, the model seemed to fit the data less well so I took out the new genre variable that I created.&lt;/p&gt;
&lt;p&gt;After performing log and exponential transformations to the dependent and some of the independent variables, the model seemed to be more or less set (at least in terms of fit of the residuals and the adjusted r-squared ~0.70). &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;model4&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;smf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ols&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;exponential(Adjusted_Gross,0.1) ~ exponential(Avg_per_Theater, 0.3) + Genre + Popular_Month + exponential(Runtime, 0.5)&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model_data_3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;result4&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model4&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_regularized&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In the above code example, the &lt;code&gt;Popular_Month&lt;/code&gt; independent variable is a generated binary variable based on which of the months were popular (May, June, July, November, and December).&lt;/p&gt;
&lt;p&gt;Given that the model was explaning about ~30% more of the variance than the initial model, I felt like it was time to move to scikit-learn to do some train test splits along with cross validation to make sure that the model was performing as expected on unseen data. &lt;/p&gt;
&lt;h2&gt;Move to scikit-learn&lt;/h2&gt;
&lt;p&gt;The main reason for moving to scikit-learn at this point was to take advantage of the convenient cross validation tools.&lt;/p&gt;
&lt;p&gt;The move to scikit-learn meant that I needed to make sure that the features were properly processed so that sklearn could take it all in. Unlike statsmodels, dummy variables are not automatically generated in sklearn so it is actually easier to generate the dummy variables using pandas.get_dummies() function. However, I made sure to delete one of the columns from each column that I was turning into dummy variables so there would not be collinearity issues.&lt;/p&gt;
&lt;p&gt;Another benefit of using scikit-learn is that the &lt;a href="http://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model"&gt;regularized linear models&lt;/a&gt; are accessible and quite easy to use. I experimented with lasso, ridge, and eventually decided to use elasticnet because it makes use of both lasso and ridge regularization. (Also scikit-learn bundles cross validation and elasticnet which makes scoring and testing the models quite simple!)&lt;/p&gt;
&lt;p&gt;In order to do cross validation, I used sklearn's &lt;code&gt;train_test_split&lt;/code&gt; module to create 70%/30% splits of the ~900 or so observations. Then I trained the model on the 70% and performed 10-fold ElasticNetCV (with sklearn) and separately tested on the remaining 30%.&lt;/p&gt;
&lt;h1&gt;What Did I Learn&lt;/h1&gt;
&lt;p&gt;Throughout the process, I realized that I was going back and forth between many of the different models and features to figure out what combination worked the best. While I could probably have used grid search or something similar to find the best parameters and automatically build a model, it was fun to manually work through building the model step by step.&lt;/p&gt;
&lt;p&gt;I also learned how to use BeautifulSoup to scrape data from a web page. While BeautifulSoup is a great package, I learned that it was still quite time consuming to figure out where the data actually resides.&lt;/p&gt;
&lt;h1&gt;Next Steps&lt;/h1&gt;
&lt;p&gt;Given that this project encapsulated many different aspects, I think that the main improvements and ways to continue learning is to refine each of the steps. For example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Web scraping: I used BeautifulSoup to scrape the web page, but I want to think about using Selenium to scrape web pages that might not have nicely laid out HTML (think information kidden behind a more interactive dynamic javascript page).&lt;/li&gt;
&lt;li&gt;Modeling: most of the modeling building and analyses that I performed in statsmodels and sklearn had to do with linear regression. However, there were many different features of both packages that I didn't get a chance to look at. I think it would be interesting to continue utilizing both packages for futher analyses.&lt;ul&gt;
&lt;li&gt;statsmodels: I performed some initial trend and seasonal decomposition in order to do time series analysis (see detrended and deseasoned graphs below!) but as mentioned earlier I decided against further looking into this for the time being.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Seasonal Decomposition" src="/images/seasonal_decompose.png"&gt;&lt;/p&gt;
&lt;p&gt;This was a fun project that exposed me to a lot of different tools and I look forward to learning more by continuing some of the ideas that I had about how to extend the analysis!&lt;/p&gt;</content></entry><entry><title>Setting up the blog</title><link href="http://jeffwen.github.io/2015/11/28/pelican-setup" rel="alternate"></link><published>2015-11-28T13:03:00-05:00</published><updated>2015-11-28T13:03:00-05:00</updated><author><name>Jeff Wen</name></author><id>tag:jeffwen.github.io,2015-11-28:/2015/11/28/pelican-setup</id><summary type="html">&lt;p&gt;Setting up the blog and things to remember&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;November 28, 2015&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Over the past couple of days, I have been working towards getting a blog up and running. There were a couple of different options that I was considering and ultimately I ended up choosing to use &lt;a href="http://blog.getpelican.com"&gt;Pelican&lt;/a&gt; as my site generator. I also use GitHub Pages to host my blog for free!&lt;/p&gt;
&lt;h3&gt;Decisions&lt;/h3&gt;
&lt;p&gt;Initially, I was considering just sticking with Jekyll, which is the site generator that pairs really nicely with &lt;a href="https://help.github.com/articles/using-jekyll-with-pages/"&gt;GitHub Pages&lt;/a&gt;. However, I read a couple different posts by other users, thought about the decision, and noticed the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Ruby vs. Python&lt;/em&gt;: Jekyll is Ruby based whereas Pelican is Python based.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Although I am also picking up Ruby, I am definitely more comfortable with Python at this point.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Speed&lt;/em&gt;: While I am still new to blogging and don't necessarily feel the struggle of slow page loading (given that I don't have much experience with slow loading blogs), &lt;a href="http://arunrocks.com/moving-blogs-to-pelican/"&gt;this user's&lt;/a&gt; blog post was definitely persuasive enough for me to take speed into consideration.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Personal Growth&lt;/em&gt;: Jekyll might have been easier to use given that it is integrated with GitHub so well, but I wanted to (at least in my mind at the time) learn a little bit more by using something that required just a little more work. &lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Getting Started&lt;/h3&gt;
&lt;p&gt;With these things in mind, I decided to jump right in and start setting up my blog. It was quite simple for the most part but it definitely took some time figuring everything out because I was trying to use a couple different resources to make sure that my blog would run smoothly. Looking back on it, I guess it probably would have been better to stick with one solid resource vs. trying to merge different ways of doing the same thing. However, as stated before, I wanted to learn as much as possible and see the different ways people were setting up their blogs.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Resources:&lt;ul&gt;
&lt;li&gt;&lt;a href="http://docs.getpelican.com/en/3.6.3/"&gt;Pelican Documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://mathamy.com/migrating-to-github-pages-using-pelican.html"&gt;Amy Halon's blog post on GitHub Pages Migration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://fedoramagazine.org/make-github-pages-blog-with-pelican/"&gt;Fedora Magazine's blog post on GitHub Pages and Pelican&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As I was following some of the resources mentioned earlier, I realized that it was a great opportunity for me to also learn more about other technologies as well. For example, I read up on GitHub submodules because Fedora's post mentioned initiaizing the output directory as a submodule. I am getting my hands dirty with Markdown because I am using Markdown to write these blog posts and pages. Additionally, HTML and CSS are relevant as well because I am learning to customize my blog. Therefore, all in all, things seem to be going pretty smoothly and I am excited to see what else I can do with this site!&lt;/p&gt;
&lt;h3&gt;Customization&lt;/h3&gt;
&lt;h4&gt;New Posts&lt;/h4&gt;
&lt;p&gt;In order to get more familiar with the workings of the platform/ try to customize what I was doing, I decided to write my own function to help speed up the process of writing a new blog. Pelican makes this quite simple with the fabfile.py, which allows you to create new functions for anything that you may want to customize.&lt;/p&gt;
&lt;p&gt;In my case, I wanted to be able to create new posts from the command line. So by typing the following I can now create a new Markdown file in my content folder with the title, slug, date, etc. preformatted.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;fab newpost:&lt;span class="s2"&gt;&amp;quot;title of my post&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The code that makes this happen is just a simple function that fills in a prespecified template.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;TEMPLATE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="s2"&gt;Title: {title}&lt;/span&gt;
&lt;span class="s2"&gt;Date: {year}-{month}-{day} {hour}:{minute}&lt;/span&gt;
&lt;span class="s2"&gt;Category:&lt;/span&gt;
&lt;span class="s2"&gt;Slug: {slug}&lt;/span&gt;
&lt;span class="s2"&gt;Summary:&lt;/span&gt;
&lt;span class="s2"&gt;Status: draft&lt;/span&gt;
&lt;span class="s2"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;newpost&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;today&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;today&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;slug&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lower&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;replace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;_&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;f_create&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;content/{}{:0&amp;gt;2}{:0&amp;gt;2}_{}.md&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;today&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;year&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;today&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;month&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;today&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;day&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;slug&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;TEMPLATE&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                &lt;span class="n"&gt;year&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;today&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;year&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                &lt;span class="n"&gt;month&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;today&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;month&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                &lt;span class="n"&gt;day&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;today&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;day&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                &lt;span class="n"&gt;hour&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;today&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hour&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                &lt;span class="n"&gt;minute&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;today&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;minute&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                &lt;span class="n"&gt;slug&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;slug&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f_create&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;w&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
         &lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;File created -&amp;gt; &amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;f_create&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Themes&lt;/h4&gt;
&lt;p&gt;I also wanted to experiment with HTML/ CSS and modify the themes that I was using on my blog. I started with the &lt;a href="https://github.com/gfidente/pelican-svbhack"&gt;SVBHACK&lt;/a&gt; theme but did not really like that the index and archives links were at the top of the page, which seemed to clutter the simplicity of the page. This was a pretty simple customization that I fixed by changing some of the HTML template code to move the links around but now the top of the page is clean!&lt;/p&gt;
&lt;p&gt;Original:&lt;/p&gt;
&lt;p&gt;&lt;img alt="original index and archives" src="/images/index_archives_orig.png"&gt;&lt;/p&gt;
&lt;p&gt;Modified:&lt;/p&gt;
&lt;p&gt;&lt;img alt="new archives" src="/images/archives_new.png"&gt;&lt;/p&gt;
&lt;p&gt;I was also interested in customizing the color of the side column and so that took some time digging through the style sheet to make it look the way I wanted. Anyways, now things seem to be the way I want them and I hope that they don't break.&lt;/p&gt;
&lt;h3&gt;Writing and Viewing&lt;/h3&gt;
&lt;p&gt;Pelican actually makes it surprisingly simple to write and view what has been written.&lt;/p&gt;
&lt;p&gt;With the function that I wrote above, I can now create first drafts pretty quickly, then when I want to see the post on my blog, Pelican allows for a local server to be created to view the changes. What is most awesome is that if I use&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;make devserver
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;I can actually have the page regenerate each time with new content and all I have to do is refresh the page. So now I can make sure that everything is formatted correctly before I publish my posts.&lt;/p&gt;
&lt;h3&gt;Final Thoughts&lt;/h3&gt;
&lt;p&gt;As I learn more about Pelican and blogging in general, I may update this post to include new information about customizations or features that I learn about!&lt;/p&gt;</content></entry><entry><title>Hello World!</title><link href="http://jeffwen.github.io/2015/11/27/hello-world" rel="alternate"></link><published>2015-11-27T03:15:00-05:00</published><updated>2015-11-27T03:15:00-05:00</updated><author><name>Jeff Wen</name></author><id>tag:jeffwen.github.io,2015-11-27:/2015/11/27/hello-world</id><summary type="html">&lt;p&gt;First blog post&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;November 27, 2015&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Hello everyone, my name is Jeff. I've started this blog to keep track of the things that I am up to and also to share about exciting things that I am learning. Thanks for stopping by!&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Hello World!&amp;#39;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</content></entry></feed>