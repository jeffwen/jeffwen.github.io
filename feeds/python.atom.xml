<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Jeff Wen - Python</title><link href="http://jeffwen.com/" rel="alternate"></link><link href="http://jeffwen.com/feeds/python.atom.xml" rel="self"></link><id>http://jeffwen.com/</id><updated>2017-08-16T15:02:00-04:00</updated><entry><title>Vehicle Detection</title><link href="http://jeffwen.com/2017/08/16/vehicle_detection" rel="alternate"></link><published>2017-08-16T15:02:00-04:00</published><updated>2017-08-16T15:02:00-04:00</updated><author><name>Jeff Wen</name></author><id>tag:jeffwen.com,2017-08-16:/2017/08/16/vehicle_detection</id><summary type="html">&lt;p&gt;Computer vision and machine learning for vehicle identification and tracking&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;August 16, 2017&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In this project, the main goal was to use computer vision and machine learning techniques to identify and track vehicles in images and video feeds. Also, take a look at the &lt;a href="https://github.com/jeffwen/sdcnd_vehicle_detection/blob/master/Vehicle%20Detection.ipynb"&gt;Jupyter Notebook&lt;/a&gt; for more details. Check out the final video output &lt;a href="https://vimeo.com/221974847"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Specifically, the project consisted of the following steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Perform a Histogram of Oriented Gradients (HOG) feature extraction on a labeled training set of images and train Linear SVM and Random Forest classifiers to compare the performance differences&lt;/li&gt;
&lt;li&gt;Apply a color transform and append binned color features, as well as histograms of color, and append to your HOG feature vector&lt;/li&gt;
&lt;li&gt;Normalize the features and randomize a selection for training and testing&lt;/li&gt;
&lt;li&gt;Implement a sliding-window technique and use the trained classifier to search for vehicles in images&lt;/li&gt;
&lt;li&gt;Run the pipeline on a video stream and create a heat map of recurring detections frame by frame to reject outliers and follow detected vehicles&lt;/li&gt;
&lt;li&gt;Estimate a bounding box for vehicles detected&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Getting Started&lt;/h2&gt;
&lt;p&gt;There are many features that we can consider extracting from an image or a video frame. We can first take a look at the images that we have in the training set (vehicles images can be found &lt;a href="https://s3.amazonaws.com/udacity-sdc/Vehicle_Tracking/vehicles.zip"&gt;here&lt;/a&gt; and non-vehicle images can be found &lt;a href="https://s3.amazonaws.com/udacity-sdc/Vehicle_Tracking/non-vehicles.zip"&gt;here&lt;/a&gt;) from to get a sense for what might work well. The images are compiled from both the &lt;a href="http://www.gti.ssr.upm.es/data/Vehicle_database.html"&gt;GTI vehicle image database&lt;/a&gt; and the &lt;a href="http://www.cvlibs.net/datasets/kitti/"&gt;KITTI vision benchmark suite&lt;/a&gt;. In addition to the data mentioned, I also supplemented the training data with labeled images from &lt;a href="http://bit.ly/udacity-annoations-crowdai"&gt;CrowdAI&lt;/a&gt;. This added about ~60k images of vehicles (both front and rear of the vehicle).&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/images/sdcnd/vehicles.png" title="vehicles"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/images/sdcnd/non_vehicles.png" title="non_vehicles"&gt;&lt;/p&gt;
&lt;p&gt;Very clearly, there are images of vehicles and non-vehicles. The object is to see if we can train a machine learning algorithm to distinguish the feaures of vehicles.&lt;/p&gt;
&lt;p&gt;As expected, the images above show cars are various zooms, crops, and angles while the non-vehicle images show scenes do not contain any cars. We can use these images to extract relevant features, which will ultimately be used to train our classifier.&lt;/p&gt;
&lt;h2&gt;Extracting Features&lt;/h2&gt;
&lt;h3&gt;Histogram of Gradients (HOG), Spatial Binning, and&lt;/h3&gt;
&lt;p&gt;First, we can try to extract the histogram of gradients. The gradients of an image will help to identify the structures within the image. On the otherhand, if we use color, it might be difficult to extract the relavant features because the same model car can be different colors. The gradient is able to capture the edges of the shape of the car. We use a modified version that averages the gradients across multiple cells to account for some possible noise in the image.&lt;/p&gt;
&lt;p&gt;Specifically, we can use the &lt;code&gt;scikit-image&lt;/code&gt; implementation of the HOG extraction function. There are a couple of parameters that we had to adjust to get meaningful features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;orientations&lt;/code&gt;: represents the number of orientation bins that the gradient information will be split up into in the histogram&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pixels_per_cell&lt;/code&gt;: specifies the cell size over which each gradient histogram is computed&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cells_per_block&lt;/code&gt;: specifies the local area over which the histogram counts in a given cell will be normalized&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="alt text" src="/images/sdcnd/hog_examples.png" title="hog_examples"&gt;&lt;/p&gt;
&lt;p&gt;Awesome, we can look closely at the HOG visualizations and see that the gradients seem to capture the shape of the vehicles quite well. If we compare the HOG visualizations of the vehicles vs. the non-vehicles, we can see that there is a difference between the different types of images.&lt;/p&gt;
&lt;p&gt;In order to find suitable values for the various parameters, we can approach the problem as a feature engineering problem and use cross validation in the model evaluation step to gauge which parameters work best. Next, we can explore another set of features: color.&lt;/p&gt;
&lt;h3&gt;Color Histogram&lt;/h3&gt;
&lt;p&gt;We can imagine that there are different shaped histograms if we look at different colored cars. However, if we go one level deeper and use a color space that is able to differentiate between cars and non-cars, this would be even more helpful in the feature space. Specifically, in some cases the vehicles are usually more saturated when compared to a pale background. In the end after trying different color spaces, the &lt;code&gt;LUV&lt;/code&gt; color space ended up performing the best when evaluated using cross validation (note that there are many other features in the feature set.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/images/sdcnd/color_hist.png" title="color_hist"&gt;&lt;/p&gt;
&lt;p&gt;The above example used the typical &lt;code&gt;RGB&lt;/code&gt; color space, but the model was trained using images converted to the &lt;code&gt;LUV&lt;/code&gt; color space.&lt;/p&gt;
&lt;h3&gt;Spatial Binning&lt;/h3&gt;
&lt;p&gt;We can also take into account the raw pixel values. However, there might be too many features if we do that so we can reduce the size of the image to return a slightly lower resolution image with the same features.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/images/sdcnd/spatial_bin.png" title="spatial_bin"&gt;&lt;/p&gt;
&lt;p&gt;We can see that in the above image the vehicle image on the right has a lower resolution but still captures most of the information that we see in the original image.&lt;/p&gt;
&lt;p&gt;At this point we have quite a few techniques to extract both color and gradient information from the images. We can define a pipeline to extract these features. Note that if and when we combine the color and gradient features we need to normalize and scale the features so that the different scales of the features do not adversely affect the classifier that we will build.&lt;/p&gt;
&lt;p&gt;The final set of parameters used in the various feature extraction methods are listed below:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Parameter&lt;/th&gt;
&lt;th align="center"&gt;Value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Color Space&lt;/td&gt;
&lt;td align="center"&gt;&lt;code&gt;LUV&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Orientations&lt;/td&gt;
&lt;td align="center"&gt;8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Pixels per Cell&lt;/td&gt;
&lt;td align="center"&gt;8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Cells per Block&lt;/td&gt;
&lt;td align="center"&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;HOG Channel&lt;/td&gt;
&lt;td align="center"&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Spatial Bin Size&lt;/td&gt;
&lt;td align="center"&gt;16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;# of Color Bins&lt;/td&gt;
&lt;td align="center"&gt;32&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;Classification&lt;/h2&gt;
&lt;p&gt;With the features extracted, we can build a classfier to identify whether the object in the image is a vehicle or not. The details and the code for this section can be found in the &lt;a href="https://github.com/jeffwen/sdcnd_vehicle_detection/blob/master/Vehicle%20Detection.ipynb"&gt;Jupyter Notebook&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To start with, we can use a Support Vector Machine Classifier, which is fairly quick and accurate when it comes to the features that we extracted. The final Linear SVM model achieved an accuracy of 98.36% without significant parameter tuning. However, in the end we used a Random Forest model that was optimized using grid search over the various parameters to achieve a performance of 99.39% on the test set.&lt;/p&gt;
&lt;p&gt;With slightly more time we could have further refined the model; however, the model wa already performing fairly well. Beyond tuning the parameters using grid search, the CrowdAI image data set also helped boost the performance of the model.&lt;/p&gt;
&lt;h3&gt;Sliding Window Search&lt;/h3&gt;
&lt;p&gt;Now that the classifier can identify if an image or a frame of a video contains a car, we need to find a way to look through the image to search for potential car matches so that we can run it against our classfier. One idea is to use a sliding window approach to scan through the image to identify possible car-like objects, then pass it to our classifier.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/images/sdcnd/search_windows_separate.png" title="search_windows_separate"&gt;&lt;/p&gt;
&lt;p&gt;The above image shows the different scale windows that we use to identify cars. Note that the image above shows windows that are overlapping so there are actually many more windows that we are searching through. Each of the windows represents a snapshot that then gets passed through to the classifier. The 3 different images above show different scales because a closer car is larger while a farther car is smaller. The different scales help to capture cars wherever they are in the image.&lt;/p&gt;
&lt;p&gt;After running each of the windows through a classifier, we are able to identify the windows where a vehicle is present.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/images/sdcnd/located_cars.png" title="located_cars"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/images/sdcnd/vehicle_windows.png" title="vehicle_windows"&gt;&lt;/p&gt;
&lt;p&gt;Cool! It seems like the classifier works fairly well. It was able to pick up where the cars are in each of the test images. However, beyond just the obvious cars there are also some incorrect classifications. Furthermore, while the above described method works and performs fairly well, it is slow because it reruns the HOG extraction every time there is a new window.&lt;/p&gt;
&lt;p&gt;A better implementation would run the HOG extraction once and reuse the extracted data by "sliding" a window across the initially extracted matrix. The below images show the output of this improved implementation.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/images/sdcnd/vehicle_final_efficient.png" title="vehicle_final_efficient"&gt;&lt;/p&gt;
&lt;p&gt;In order to deal with the false positives, we can count the number of times that a pixel is contained within a bounding box and use that to create a heatmap of which pixels are most likely to be of a vehicle.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/images/sdcnd/vehicle_heatmap.png" title="vehicle_heatmap"&gt;&lt;/p&gt;
&lt;p&gt;The heatmap strategy works fairly well to control for the noise in our images. Now, we can use the scipy &lt;code&gt;label&lt;/code&gt; function to identify where the non-zero observations or the 'heated' observations are.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/images/sdcnd/heatmap.png" title="heatmap"&gt;&lt;/p&gt;
&lt;p&gt;The image of the left shows the final output with the hotspots surrounded by a bounding box. The image on the right shows the heatmap of which pixels are contained within the most windows. Bringing this all together we can now identify the areas where a vehicle is most likely to be.&lt;/p&gt;
&lt;p&gt;When applying this pipeline to videos, we need to deal with jitter that is introduced frame to frame. In some cases the pipeline might not be able to identify a vehicle in the frame. One method for dealing with this is to store a history of the bounding boxes. Then, for each frame we can run a heatmap of the bounding boxes to get a smoother version of the bounding boxes. Ultimately, this step along with the heatmap creation mentioned above helps in accounting for false positives, because the final output is an average of the last couple of frames.&lt;/p&gt;
&lt;h2&gt;Discussion&lt;/h2&gt;
&lt;p&gt;Overall, this project was quite interesting and required techniques from both computer vision and machine learning. It was a joy to finally see the algorithm identify cars in the video feed. However, there were definitely challenges and there is room for improvement.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Performance&lt;/strong&gt;: One of the major drawbacks of the current pipeline is that it is relatively slow. Even though the classification of the frame is quick, the extraction of the features is slow. An improvement to the speed of the extraction could mean that this pipeline could be applied to an actual real-time feed. Currently the rate of feature extraction and classification is roughly 1.5 frames per second.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Noise&lt;/strong&gt;: Although the current pipeline does a fairly good job at identifying the cars, in some cases there are still misclassifications. Ideally these misclassifications need to be dealt with so that the classifications don't lead to our self-driving car crashing!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Feature Extraction&lt;/strong&gt;: The process of identifying the right features to extract is really difficult and we can actually imagine that using a convolutional neural network might help in extracting the relevant features. A CNN is able to identify features that are important without much supervision. This might perform better than explicitly setting up the pipeline to extract the HOG, color histogram, and raw pixel values. Additionally, a car that is oout of the ordinary would be an issue for this pipeline.&lt;/li&gt;
&lt;/ul&gt;</content></entry><entry><title>Behavioral Cloning</title><link href="http://jeffwen.com/2017/07/14/behavioral_cloning" rel="alternate"></link><published>2017-07-14T11:07:00-04:00</published><updated>2017-07-14T11:07:00-04:00</updated><author><name>Jeff Wen</name></author><id>tag:jeffwen.com,2017-07-14:/2017/07/14/behavioral_cloning</id><summary type="html">&lt;p&gt;Cloning driving behavior with convolutional neural networks&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;July 24, 2017&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Here is the final two corners of the track with the completed model!&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/images/sdcnd/final.gif" title="Final Run"&gt;&lt;/p&gt;
&lt;p&gt;In this project, we design, train, and test a convolutional neural network (CNN) to clone the driving behavior from sample images recorded from &lt;a href="https://github.com/udacity/self-driving-car-sim"&gt;Udacity's driving simulator&lt;/a&gt;. The code for this project can be found at this &lt;a href="https://github.com/jeffwen/sdcnd_behavioral_cloning"&gt;repository&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;The goals / steps of this project are the following:
&lt;em&gt; Use the simulator to collect data of good driving behavior
&lt;/em&gt; Build, a convolution neural network (using Keras) that predicts steering angles from images
&lt;em&gt; Train and validate the model with a training and validation set
&lt;/em&gt; Test that the model successfully drives around the track without leaving the road&lt;/p&gt;
&lt;p&gt;The CNN that was eventually used was based on NVIDIA's &lt;em&gt;&lt;a href="https://arxiv.org/pdf/1604.07316v1.pdf"&gt;End to End Learning for Self-Driving Cars&lt;/a&gt;&lt;/em&gt; paper with different input image size and with dropout added to improve robustness. Most of the training was done using an Amazon g2.2xlarge EC2 instance, which helped make training much quicker.&lt;/p&gt;
&lt;h3&gt;Data Gathering&lt;/h3&gt;
&lt;p&gt;One of the most time consuming aspects of this project was collecting the adequate data to train on. While Udacity provided a dataset from the training simulator, it was more exciting to collect my own training data. Below is a sample of the simulator screen. When collecting data, we use the "Training Mode." There are also two tracks available for training and testing. I focused on using the first track. The second image shows the view that the user sees in the driving simulator.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/images/sdcnd/simulator_screen.png" title="Simulator Screen"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/images/sdcnd/simulator_car.png" title="Simulator Car"&gt;&lt;/p&gt;
&lt;p&gt;By using the simulator that was provided, it was easy to record laps that would be stored as separate frames. However, the data gathering process had to include a lot of "recovery driving" and driving around corners to make sure that the model could learn the correct behaviors. After recording the different simulator runs, the output are individual frames.&lt;/p&gt;
&lt;h3&gt;Data Preparation&lt;/h3&gt;
&lt;p&gt;The final dataset before augmentation consisted of 7986 training observations and 1997 validation observations. However, the simulator actually not only recorded the center image, but also the left and right viewpoints. As seen below, the 3 camera angles comprise of each frame of the video. Therefore, the data quantitfy can actually be increased by making use of the left and right images. In particular, when using the left and right camera angles, I had to correct the steering angle so that the data could represent what the steering angle would have been if the viewpoint was from the left or right camera angles.&lt;/p&gt;
&lt;p&gt;The data took some fiddling to figure out what worked eventual specifications. For example, when intially training the model, the images were not cropped (160x320x3 images), which meant that for every image there was a large percentage of the image that contained useless information for informing the steering angle. &lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/images/sdcnd/original_simulator_image.png" title="Original Simulator Image"&gt;&lt;/p&gt;
&lt;p&gt;In the above example, we can see that the top portion of every image and even the bottom portion of the image contained extraneous information. After cropping, the performance of the model was improved.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/images/sdcnd/cropped_simulator_image.png" title="Cropped Simulator Image"&gt;&lt;/p&gt;
&lt;p&gt;Futhermore, the NVIDIA paper used the YUV channel in their implementation and I similarly chose to use the YUV channel after experimenting between RGB, BGR, and the YUV channels. &lt;/p&gt;
&lt;h3&gt;Data Exploration and Augmentation&lt;/h3&gt;
&lt;p&gt;In order to get the network to perform well, I first ran roughly 2 laps around the track (one in each direction). However, the model automated driving was quite horrible on the curved portions of the road. The car would drive off the road fairly frequently. &lt;/p&gt;
&lt;p&gt;I took a closer look at the data distribution after reading that this might be a potential issue (the model doesn't know how to deal with corners).&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/images/sdcnd/original_distribution.png" title="Original Distribution"&gt;&lt;/p&gt;
&lt;p&gt;As seen in the image above, the distribution of the data is heavily skewed towards the very low steering angles, which makes sense because most of the driving was on relatively straight roads vs. curved roads. In order to try to create a more even distribution, I collected roughly half a lap (total frames) of curved road driving and also about a quarter of a lap of recovery driving. The recovery driving entailed hitting the record button when I intentionally approached a corner and did not turn in time, then turning heavily to "correct" the behavior. &lt;/p&gt;
&lt;p&gt;However, in addition to manually trying to augment data I also forced the distribution to be more uniform by downsampling the over represented steering angles and upsampling the underrepresented ones. I created a histogram and randomly chose up to observations that fell into a particular bucket to get the total number of observations up to 500.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/images/sdcnd/augmented_distribution.png" title="Augmented Distribution"&gt;&lt;/p&gt;
&lt;p&gt;The resulting data distribution was much more uniform and resulted in the car being able to correct for even severe mistakes (such as driving too far to one side of the track and hitting the apex). The below gif shows the model trained using the augmented data. We can see that even though the vehicle gets close to the apex/side of the road, it is able to navigate back to the center of the track.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/images/sdcnd/recovery.gif" title="Recovery"&gt;&lt;/p&gt;
&lt;h3&gt;Building the Model&lt;/h3&gt;
&lt;p&gt;After getting the data ready, I focused on replicating the NVIDIA model architecture because it had worked well in the real life setting and therefore I figured it would be useful in the simulated world as well. &lt;/p&gt;
&lt;p&gt;Specifically, the network is 9 layers deep with exponential activation units and dropout after the convolutional layers. As mentioned previously, the training took place on a Amazon EC2 instance and I use 5 epochs to train on 80% of the total data with 20% left for validation. The network was built with Keras and used Tensorflow as the backend. One of the awesome things about Keras is that it keeps the model building relatively simple so the model was written with only a couple lines of code.&lt;/p&gt;
&lt;p&gt;While the network worked, after reading other student's efforts it seems like even simpler networks could work quite well.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Layer&lt;/th&gt;
&lt;th align="center"&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Input&lt;/td&gt;
&lt;td align="center"&gt;80x320x3 YUV Image&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Normalization&lt;/td&gt;
&lt;td align="center"&gt;Normalize batch&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Convolution 5x5&lt;/td&gt;
&lt;td align="center"&gt;2x2 stride, valid padding, outputs 38x158x24&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;ELU activation&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Convolution 5x5&lt;/td&gt;
&lt;td align="center"&gt;2x2 stride, valid padding, outputs 17x77x36&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;ELU activation&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Convolution 5x5&lt;/td&gt;
&lt;td align="center"&gt;2x2 stride, valid padding, outputs 7x37x48&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;ELU activation&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Convolution 3x3&lt;/td&gt;
&lt;td align="center"&gt;1x1 stride, valid padding, outputs 5x35x64&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;ELU activation&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Convolution 3x3&lt;/td&gt;
&lt;td align="center"&gt;1x1 stride, valid padding, outputs 3x33x64&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;ELU activation&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Dropout&lt;/td&gt;
&lt;td align="center"&gt;0.5 keep probablility (training)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Flatten&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Fully connected&lt;/td&gt;
&lt;td align="center"&gt;3168 input, 100 output&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Fully connected&lt;/td&gt;
&lt;td align="center"&gt;100 input, 50 output&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Fully connected&lt;/td&gt;
&lt;td align="center"&gt;50 input, 10 output&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Output&lt;/td&gt;
&lt;td align="center"&gt;10 input, 1 output&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In order to feed data to the network, I used a generator function so that the data did not have to all be stored in memory. Only when the data was needed would it be read into memory and sent over as a batch.&lt;/p&gt;
&lt;h3&gt;Discussion and Next Steps&lt;/h3&gt;
&lt;p&gt;Overall, this project was really interesting and it was surprising that a couple lines of code could create a model that was able to emulate driving behavior. While the results are acceptable, there are definitely a couple improvements that come to mind.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;First of all, the data gather process could have been more thorough and also could have used different samples. In this case, I trained the model on images from the same track that it eventually ran on. It would interesting to tune the model and make it robust enough to work on any track that it is set on.&lt;/li&gt;
&lt;li&gt;In terms of data preprocessing, only very simple data preprocessing steps were taken. It might be beneficial to combine some of the steps that were implemented in the other projects to help, for example, highlight the lane lines more clearly so that the model can use less noisy input data.&lt;/li&gt;
&lt;li&gt;In reality, a vehicle would not be running in isolation and instead would be in an environment where there are many other distractions and unexpected objects that might suddenly appear. This is where radar and sensor data could be used in combination with the computer vision aspects to help inform a more well rounded model. &lt;/li&gt;
&lt;/ul&gt;</content></entry><entry><title>Advanced Lane Finding</title><link href="http://jeffwen.com/2017/06/18/advanced_lane" rel="alternate"></link><published>2017-06-18T20:58:00-04:00</published><updated>2017-06-18T20:58:00-04:00</updated><author><name>Jeff Wen</name></author><id>tag:jeffwen.com,2017-06-18:/2017/06/18/advanced_lane</id><summary type="html">&lt;p&gt;Using computer vision techniques to refine the identification of lane lines in both images and video feeds&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;June 18, 2017&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The main objective for this project was to take a video feed from an onboard camera and identify the lane lines and the curvature of the road. The below image presents an example of one frame of the output that is expected. This projected uses different computer vision techniques to extract the lane lines in the frames. Code for this project can be found in this &lt;a href="https://github.com/jeffwen/sdcnd_advanced_find_lanes/blob/master/Advanced%20Lane%20Finding.ipynb"&gt;Jupyter Notebook&lt;/a&gt; and the video for this project can be found &lt;a href="https://vimeo.com/218746417"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/images/sdcnd/summary_lane_area.png" title="summary_lane_area"&gt;&lt;/p&gt;
&lt;p&gt;The goals / steps of this project are the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Compute the camera calibration matrix and distortion coefficients given a set of chessboard images&lt;/li&gt;
&lt;li&gt;Apply a distortion correction to raw images&lt;/li&gt;
&lt;li&gt;Use color transforms, gradients, and sobel filters to create a thresholded binary image&lt;/li&gt;
&lt;li&gt;Apply a perspective transform to rectify binary image ("birds-eye view")&lt;/li&gt;
&lt;li&gt;Detect lane pixels and fit to find the lane boundary&lt;/li&gt;
&lt;li&gt;Determine the curvature of the lane and vehicle position with respect to the center of the lane&lt;/li&gt;
&lt;li&gt;Warp the detected lane boundaries back onto the original image&lt;/li&gt;
&lt;li&gt;Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Camera Calibration&lt;/h2&gt;
&lt;p&gt;One of the first steps to the project is to take in the images from the forward facing camera. However, these images typically have some distortion that needs to be accounted for if we want to later be able to calculate the curvature of the lane lines. Specifically, this has to do with the lenses that are in the cameras used to take pictures.&lt;/p&gt;
&lt;p&gt;One of the ways to address this distortion is to calibrate our camera if we know how much the lenses distort our image. To start with, we can take a look at a chessboard image (we know how it should actually look with no distortion) before we calibrate and undistort the image. This will allow us to calculate how much distortion is present and correct for it (most of the heavy lifting is done by OpenCV). We can do this same calibration with a set of chessboard images. Below are a couple images without undistortion.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/images/sdcnd/orig_distorted_img.png" title="orig_distorted_img"&gt;&lt;/p&gt;
&lt;p&gt;As seen in the above examples, the images are quite varied which allows us to correct for all types of images that the camera will eventually see.&lt;/p&gt;
&lt;p&gt;Now, we can start to calibrate the camera. One way of doing this is to map points on the distorted image to an undistorted image (where we know all the location of those points, then we can transform or undistort the original image. In the case of using chessboards, the intersection of the black and white squares is a really good place to start.&lt;/p&gt;
&lt;p&gt;We can use the OpenCV package to find these intersection points using the &lt;code&gt;findChessboardCorners&lt;/code&gt; function &lt;a href="http://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html#findchessboardcorners"&gt;read more here&lt;/a&gt;. This will return a set of detected corner coordinates if the status is &lt;code&gt;1&lt;/code&gt; or no corners if no corners could be found (status of &lt;code&gt;0&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;![alt text]corners_img]&lt;/p&gt;
&lt;p&gt;As we can see, the function did a pretty good job of identifying the internal corners in the image of the chessboard. Now we need to undistort the image by using a mapping of these found corners our understanding of how a chessboard usually looks.&lt;/p&gt;
&lt;p&gt;First we need to store these points so that we can use the points to undistort our images then we can calibrate the camera. The calibration step uses these object (actual flat chessboard) and image (distorted image) points to calculate a camera matrix, distortion matrix, and a couple other data points related to the position of the camera. We can use these outputs from the &lt;code&gt;calibrateCamera&lt;/code&gt; function in the &lt;code&gt;undistort&lt;/code&gt; function to undistort different images.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/images/sdcnd/orig_undistorted_img_1.png" title="orig_undistorted_img"&gt;
&lt;img alt="alt text" src="/images/sdcnd/orig_undistorted_img_1.png" title="orig_undistorted_img_1"&gt;&lt;/p&gt;
&lt;p&gt;We can use the same functions over all of the images (usually it is recommended to use ~20 images for calibration of the camera). With the camera matrix and the distortion matrix, we can apply the reverse distortion to the images from the forward facing camera from our car. We can take a look at an original image and an image that is corrected for the camera distortion.&lt;/p&gt;
&lt;p&gt;Below are a couple samples of the distortion correction applied to our lane line images.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/images/sdcnd/undistorted_lane_img.png" title="undistorted_lane_img"&gt;&lt;/p&gt;
&lt;p&gt;From the above lane lines images, it might be slightly difficult to see the camera distortion change, but if we look at the hood of the car or the cars in the adjacent lanes you can see that the image indeed did become undistorted.&lt;/p&gt;
&lt;h2&gt;Detecting Lane Lines&lt;/h2&gt;
&lt;p&gt;Now that we have undistorted images, we can start to detect lane lines in the images. Ultimately, we would like to calculate the curvature of the lanes so that we can decide how to steer our car.&lt;/p&gt;
&lt;p&gt;In this section, we use different color spaces, gradient, and direction thresholding techniques to extract lane lines. First of all, we can use different edge detection types of filter (similar to canny edge detection) such as the Sobel filter.&lt;/p&gt;
&lt;h3&gt;Sobel Filter Thresholding&lt;/h3&gt;
&lt;p&gt;The sobel filter takes the derivative in the &lt;code&gt;x&lt;/code&gt; or &lt;code&gt;y&lt;/code&gt; direction to highlight pixels where there is a large change in the pixel values.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/images/sdcnd/sobel_mask.png" title="sobel_mask"&gt;&lt;/p&gt;
&lt;p&gt;It seems like the x-axis definitely performs better for this task. It makes sense because the lane lines are typically vertical lines and the x-axis direction of the sobel filter highlights vertical lines (versus the y-axis direction highlights many of the horizontal elements of the image).&lt;/p&gt;
&lt;p&gt;While in the example above the lane lines are pretty clearly identified, if we look at a harder example the result is not as clean.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/images/sdcnd/challenge_sobel_mask.png" title="challenge_sobel_mask"&gt;&lt;/p&gt;
&lt;p&gt;Note that in the top example, we almost completely do not see the yellow lane line. Rather we capture the different colored pavement. In the bottom example, we also mainly capture the different pavement transition.&lt;/p&gt;
&lt;p&gt;We can use other thresholding techniques (using color spaces) to identify the lane line even in situations where the we have anomalous aspects in the input image.&lt;/p&gt;
&lt;h3&gt;Magnitude and Direction of Gradient Thresholding&lt;/h3&gt;
&lt;p&gt;One approach to potentially clean up the basic sobel filter thresholding is to use the magnitude of the &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; gradients to filter out gradients that are below a certain threshold. We can combine this with the sobel filter above to hopefully achieve better results.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/images/sdcnd/mag_mask.png" title="mag_mask"&gt;&lt;/p&gt;
&lt;p&gt;It seems like the magnitude threshold does a fairly good job but is quite similar to the sobel filter alone. Another thresholding technique we can consider is to use the direction of the gradient. In particular because we are looking for lane lines, we know that typically the lane line is a sloped line from the perspective of the onboard camera.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/images/sdcnd/dir_mask.png" title="dir_mask"&gt;&lt;/p&gt;
&lt;p&gt;Although now the direction mask seems to capture different aspects than the magnitude of the gradient or the sobel filter, there is a lot of noise in the direction mask. In the end, I ended up not using this mask because it was difficult to incorporate and did not add much value.&lt;/p&gt;
&lt;p&gt;Next, we can consider combining the different masks into one function and take a look at the output.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/images/sdcnd/combined_mask.png" title="combined_mask"&gt;&lt;/p&gt;
&lt;p&gt;Taking a look at the above output, it seems like the combined thresholding does relatively well on the simple examples, but there are still difficulties with the challenge images. Note that the noise in the rest of the image will be masked away later so we can just aim to better highlight the lane lines using color thresholding.&lt;/p&gt;
&lt;h3&gt;Color Thresholding&lt;/h3&gt;
&lt;p&gt;&lt;img alt="alt text" src="/images/sdcnd/olor_mask.png" title="color_mask"&gt;&lt;/p&gt;
&lt;p&gt;Using the color selection technique (both the HSV and the HSL color spaces) it seems like the thresholding on color now works fairly well! We can see that the lane lines are correctly identified even in the challenge images.&lt;/p&gt;
&lt;p&gt;Note that while there is a lot of noise at the top of the images, these portions will be masked out. Now we can try and combine the different thresholding techniques.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/images/sdcnd/threshold_pipeline.png" title="threshold_pipeline"&gt;&lt;/p&gt;
&lt;p&gt;Not too bad! There is a lot of noise that is introduced because of the non-color thresholding. In the end, I decided to use only the color thresholding because it returned the best lane lines.&lt;/p&gt;
&lt;h2&gt;Perspective Transform&lt;/h2&gt;
&lt;p&gt;Now we can think about how to adjust the perspective of the image so that we can detect the curvature of the lane lines. One way of doing this is to take a birds eye view of the picture (from the top down). In order to do this, we can use the &lt;code&gt;getPerspectiveTransform&lt;/code&gt; and &lt;code&gt;warpPerspective&lt;/code&gt; functions in OpenCV.&lt;/p&gt;
&lt;p&gt;By transforming the image, we are able to just focus on the part of the picture that is important to the task of finding the lane lines. In the first example below, I used the entire thresholding pipeline. If we compare this to just using the color thresholding pipeline we see that the color thresholding pipeline performs better. The remainder of this project uses the color thresholding.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/images/sdcnd/warped_1.png" title="warped_1"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/images/sdcnd/transformed_1.png" title="transformed_1"&gt;&lt;/p&gt;
&lt;p&gt;The second of the two above images shows the transformed view before and after thresholding. In order to perform the transformation, we defined source and destination points in the original image and a hypothetical output image onto which the source points are mapped. In this case, 4 points corresponding to the lane lines might be transformed to a square or rectangle shape in the output image, which then returns the lane lines in a straight image if the lane lines are indeed straight.&lt;/p&gt;
&lt;h2&gt;Sliding Window Search and Polynomial Line Fitting&lt;/h2&gt;
&lt;p&gt;Next we can try to figure out where the lane lines are and how much curvature are in the lane lines. In order to find the lane lines, we can create a histogram with the columns of the image intensities summed together. This would return higher values for areas where there are higher intensities (lane lines).&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/images/sdcnd/histogram_lane.png" title="histogram"&gt;&lt;/p&gt;
&lt;p&gt;The histogram technique worked fairly well but if we take a closer look we the lane lines (unless they are perfectly straight) will be diagonal across the image. Therefore, it would be best to take a window approach to look at segments of the image (or horizontal bands) in order to be able to capture the curvature of the lanes.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/images/sdcnd/sliding_window_fit.png" title="sliding_window_fit"&gt;&lt;/p&gt;
&lt;p&gt;In the above example, we can see that the lane lines are quite well defined and the fitted line is also quite accurate. With the poly lines drawn on the images, we can fill in the area of the image where the lane is most likely to be.&lt;/p&gt;
&lt;h2&gt;Lane Area and Curve Details&lt;/h2&gt;
&lt;p&gt;&lt;img alt="alt text" src="/images/sdcnd/lane_area.png" title="lane_area"&gt;&lt;/p&gt;
&lt;p&gt;There are also some statistics regarding the curvature of the left and right lanes &lt;a href="http://www.intmath.com/applications-differentiation/8-radius-curvature.php"&gt;read more here&lt;/a&gt;. The distance from the center of the lane is an approximate calculated using the poly fit lines extended to the bottom of the image as the width of the lane lines and the actual middle of the image as the middle of the car's onboard camera.&lt;/p&gt;
&lt;p&gt;Below are links to the final videos:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://vimeo.com/218746417"&gt;Advanced Lane Finding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://vimeo.com/218746468"&gt;Advanced Lane Finding (Challenge)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The challenge video shows some of the shortcomings of the current pipeline. While it works fairly well, there are still some unexpected inputs (previous lane line markings, shadows, and varying pavement colors) that cause the pipeline to make a few mistakes.&lt;/p&gt;
&lt;h2&gt;Discussion&lt;/h2&gt;
&lt;p&gt;Overall, this project was really interesting because it was a great extension to the first lane finding project that I worked on. However, I got to use more advanced techniques to solve the same problem. Although the project was interesting, it was also quite difficult and time consuming. Below I will discuss some of the difficulties of the project and also some of the places where the pipeline might fail.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Detecting and Removing Outliers&lt;/strong&gt; - in order to build a robust pipeline it is important to take into account the anomalies that might surface in an onboard camera feed. In a few cases there were areas of the road that was obstructed by shadows or different colored pavements. It was important to account for these anomalies by building a function that takes into account the past lane curves and averages to find the "best" fit. This helped to smooth the resulting video of the detected lane lines. Another related difficulty was figuring out what the thresholds should be for the sanity check. The sanity check function would compare the newly calculated curves with the old curves to see if frame by frame there was a large difference in the detected lanes. However, it required a lot of manual fine tuning to get reasonable thresholds for the rejection of certain fitted curves.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Thresholding&lt;/strong&gt; - A lot of the time was spent refining the thresholding functions. For the most part, the color thresholding performed the best, but it took a lot of trial and error to correctly select the colors from the different color channels. One important observation was that it is helpful to use different color spaces for retrieving different color lane lines.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Unexpected Inputs&lt;/strong&gt; - While the pipeline performs quite well on the videos given, it might not perform well on videos that have a lot of unseen elements, such as large curves and bends, uneven lighting, or lane changing situations. Ideally a self-driving car would take these into account.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Next Steps and Conclusion&lt;/h2&gt;
&lt;p&gt;The project was extremely rewarding and moving forward there are a lot of improvements that can be implemented. For example, there might be more opportunities to use machine learning to learn how to demarcate the lane lines rather than just use computer vision techniques to extract the lane lines. The difficulty with just using image processing techniques is that there might be situations that are unaccounted for (although if we have no training data a machine learning algorithm would not perform much better).&lt;/p&gt;
&lt;p&gt;I am excited to continue researching and learning about this space. There are also very interesting feature extraction techniques that I would like to explore to better be able to detect lane lines. The next step is to think about how these identified lanes can be used to control the vehicle (though there might, in reality, be a few steps before we get there).&lt;/p&gt;</content></entry><entry><title>Traffic Sign Recognition</title><link href="http://jeffwen.com/2017/03/28/traffic_signs" rel="alternate"></link><published>2017-03-28T20:41:00-04:00</published><updated>2017-03-28T20:41:00-04:00</updated><author><name>Jeff Wen</name></author><id>tag:jeffwen.com,2017-03-28:/2017/03/28/traffic_signs</id><summary type="html">&lt;p&gt;Classifying traffic signs with convolutional neural networks (LeNet architecture)&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;March 28, 2017&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In this project, we classify &lt;a href="http://benchmark.ini.rub.de/?section=gtsrb&amp;amp;subsection=dataset"&gt;German traffic signs&lt;/a&gt;. The code for this project can be found in this &lt;a href="https://github.com/jeffwen/sdcnd_traffic_sign/blob/master/Traffic_Sign_Classifier.ipynb"&gt;IPython notebook&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The objective of this project is to ultimately design, train, and test a convolutional neural network (CNN) to see if we can accurately classify the traffic signs. Specifically, with image classification tasks the variability of input images might cause the model to perform poorly. In order to remedy this situation, we will consider preprocessing steps to ensure that the model is robust to varying input images. Furthermore, depending on how the model performs, we may need to consider regularization steps such as dropouts or L2 regularization. &lt;/p&gt;
&lt;p&gt;To start with, I use the &lt;a href="http://yann.lecun.com/exdb/lenet/"&gt;LeNet-5 architecture&lt;/a&gt; that was first published by Yann Lecun's lab in 1998. Eventually, I augmented the training set with rotated and zoomed images, normalized images, and included dropouts for the fully connected layers. This led to a final model that achieved ~95% on the validation set and ~94% on the test set. &lt;/p&gt;
&lt;h3&gt;Data Set Summary &amp;amp; Exploration&lt;/h3&gt;
&lt;p&gt;Before jumping into the model, we can take a look at some of the images within the dataset. &lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/images/sdcnd/example1.png" title="80 kph"&gt; &lt;img alt="alt text" src="/images/sdcnd/example2.png" title="Straight or right"&gt; &lt;img alt="alt text" src="/images/sdcnd/example3.png" title="Stop"&gt; 
&lt;img alt="alt text" src="/images/sdcnd/example4.png" title="100 kph"&gt; &lt;img alt="alt text" src="/images/sdcnd/example5.png" title="30 kph"&gt; &lt;img alt="alt text" src="/images/sdcnd/example6.png" title="Beware ice/ snow"&gt;&lt;/p&gt;
&lt;p&gt;The images vary quite a lot in terms of brightness/ we can imagine that the orientation of the images might also be quite different. More specifically, we can take a look at just stop signs and see that there is quite a lot of variation. &lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/images/sdcnd/stop1.png" title="Stop Sign"&gt; &lt;img alt="alt text" src="/images/sdcnd/stop2.png" title="Stop Sign"&gt; &lt;img alt="alt text" src="/images/sdcnd/stop3.png" title="Stop Sign"&gt; 
&lt;img alt="alt text" src="/images/sdcnd/stop4.png" title="Stop Sign"&gt; &lt;img alt="alt text" src="/images/sdcnd/stop5.png" title="Stop Sign"&gt; &lt;img alt="alt text" src="/images/sdcnd/stop6.png" title="Stop Sign"&gt;&lt;/p&gt;
&lt;p&gt;Additionally, we can take a look at how the traffic sign classes are distributed (to see if the classes are evenly balanced). Looking at the below histogram, it seems like there are some classes that are represented 7x as much as the least  represented class. Later on we might want to upsample the less represented classes to make the model more robust. &lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/images/sdcnd/histogram.png" title="Class Histogram"&gt;&lt;/p&gt;
&lt;p&gt;Below are some summary statistics calculated from the dataset that we have:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Number of training examples = &lt;code&gt;34799&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Number of validation examples = &lt;code&gt;4410&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Number of testing examples = &lt;code&gt;12630&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Image data shape = &lt;code&gt;(32, 32)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Number of classes = &lt;code&gt;43&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Data Preprocessing and Model Evaluation&lt;/h3&gt;
&lt;p&gt;The input data is rather varied in terms of lighting and orientation of images. In order to improve the accuracy of the model, we can implement some preprocessing steps on our images. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Grayscaling, Mean Subtraction, and Normalization&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;First of all, I implemented grayscaling of the traffic sign images. While there may be some information lost because the color information in the image might contain information about the traffic sign, grayscaling simplifies the 3 channels into a single channel, which can lead to quicker model training. Additionally, taking away the color channels might allow the CNN to "focus" on extracting the important non-color features better. &lt;/p&gt;
&lt;p&gt;After performing grayscaling, I performed mean subtraction and normalization of the image pixels. Specifically, in the cs231n &lt;a href="http://cs231n.github.io/neural-networks-2/"&gt;course notes&lt;/a&gt; Andrej Karpathy mentions that the standard preprocessing techniques involve mean subtraction &lt;code&gt;img = img - np.mean(img)&lt;/code&gt; and normalization &lt;code&gt;img = img / np.std(img)&lt;/code&gt;. &lt;/p&gt;
&lt;p&gt;Before Processing Images&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/images/sdcnd/before_process.png" title="Before Processing"&gt; &lt;img alt="alt text" src="/images/sdcnd/before_process2.png" title="Before Processing"&gt; &lt;/p&gt;
&lt;p&gt;After Processing Images&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/images/sdcnd/after_process.png" title="After Processing"&gt; &lt;img alt="alt text" src="/images/sdcnd/after_process2.png" title="After Processing"&gt; &lt;/p&gt;
&lt;p&gt;At this point, model performed fairly well and better than the 93% on the validation set that was required in the grading rubric. To start with, the original LeNet-5 network applied to the colored images achieved ~88% on the validation set. Then the gray scaling, mean subtraction, and normalization combined with dropouts on the fully connected layers pushed the validation accuracy to ~95%. 
- I added the dropouts because looking at the original train vs. validation curve, the model shows strong overfitting. As to why the drops were only applied to the fully connected layers, looking at this &lt;a href="https://www.kaggle.com/c/state-farm-distracted-driver-detection/discussion/20201"&gt;kaggle question&lt;/a&gt; the author mentions that we usualy do not apply dropouts to the convolution layer because the convolution layer is meant to extract features and also does not have that many features to regularize.&lt;/p&gt;
&lt;p&gt;Original LeNet Architecture&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/images/sdcnd/train_valid_original.png" title="Train vs Validation Curve (Original)"&gt; &lt;/p&gt;
&lt;p&gt;LeNet Architecture with Grayscaling, Normalization, and Dropouts&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/images/sdcnd/train_valid_gray_norm.png" title="Train vs Validation Curve (Grayscale, Normalization, Dropout)"&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Model Evaluation and Deep Dive&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;While the model is performing fairly well, we can dive deeper in to the performance of the model to figure out what is happening with the predictions. Specifically, below are the histogram and confusion matrix of validation actuals vs. the validation predicted. By looking at the histogram and confusion matrix, we can look at where the classifications are mistaken and what classes are typically wrong. &lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/images/sdcnd/actual_vs_pred_hist.png" title="Actual vs. Predicted Histogram"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/images/sdcnd/gray_norm_confusion.png" title="Confusion Matrix (grayscale, normalization, dropounts)"&gt;&lt;/p&gt;
&lt;p&gt;If we look at the histogram above, we see that most of the bars are at about the same place. Similarly, it seems like the diagonal is quite well defined, which means that the CNN did fairly well! However, if we look at the off diagonal cells in the confusion matrix, we see that there are definitely misclassifications. For example, what is actually supposed to be class 0 gets misclassified as class 4 or class 16 gets misclassified as class 41. Below are sample images from these classes.&lt;/p&gt;
&lt;p&gt;Class 0 misclassified as Class 4&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/images/sdcnd/class0_class4_1.png" title="Class 0"&gt; &lt;img alt="alt text" src="/images/sdcnd/class0_class4_2.png" title="Class 4"&gt;&lt;/p&gt;
&lt;p&gt;Class 16 misclassified as Class 41&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/images/sdcnd/class16_class41_2.png" title="Class 16"&gt; &lt;img alt="alt text" src="/images/sdcnd/class16_class41_1.png" title="Class 41"&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Data Augmentation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;So it seems fairly reasonable, especially for the class 0 to class 4 example for the network to misclassify the images. It even looking at the gray and normalized images the differences between the two are not that drastic. In order to improve the model's performance on these, we can augment the dataset by creating more images with various operations like zooming and rotating the images. Note that it seems like the classes that are more often misclassified are also the classes with the fewest amount of data. This step took an unexpectedly long time because many additional images were created of the under represented classes. With these additional images, I retrained the model. Overall, 11915 additional images were created and a few of the additional images are shown below.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/images/sdcnd/new_gen_image.png" title="Processed Image"&gt; &lt;img alt="alt text" src="/images/sdcnd/new_gen_image_1.png" title="Processed Image"&gt;&lt;/p&gt;
&lt;p&gt;After training with the additional images and increasing the number of epochs to 25, the accuracy on the validation set increased to ~95%. The actual vs. predicted histogram and training/ validation curve also shows an improvement. If we look closely at the actual vs. predicted histogram, it seems like the classes that were misclassified previously are now more accurately classified, which means the data augmentation worked!&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/images/sdcnd/aws_histogram.png" title="Aws Histogram"&gt; &lt;img alt="alt text" src="/images/sdcnd/aws_train_valid_curve.png" title="AWS Train Curve"&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Iterative Model Building&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As evidenced above, the process of training, validating, and testing the model architecture took an extended amount of time. Mainly because after running the the model over the validation set, I would return to the preprocessing stages to experiment with various other methods for preprocessing. &lt;/p&gt;
&lt;p&gt;When fiddling with the model's hyperparameters, I had to switch over to use AWS to train the model because with the additional images and an increased number of epochs, the model was too slow to run on my local machine. Specifically, I launched a &lt;code&gt;g2.2xlarge&lt;/code&gt; Amazon EC2 instance and it took ~2-3 minutes to train on the 45,000+ image training set with 25 epochs.&lt;/p&gt;
&lt;p&gt;In terms of further improvement on the model, the training and validation curves are still fairly separated meaning the model is still overfitting. In the future, I will consider applying L2 regularization and/or increasing the dropout % to reduce the chance of overfitting.&lt;/p&gt;
&lt;h3&gt;Design and Test a Model Architecture&lt;/h3&gt;
&lt;p&gt;In terms of the model architecture that was used to achieve the results above, I used a modified version of the LeNet-5 architecture (shown below). The code used to build the CNN is under the "Model Architecture" section of the Jupyter Notebook. &lt;/p&gt;
&lt;p&gt;LeNet Architecture (Actual architecture is slightly different)&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/images/sdcnd/lenet.png" title="LeNet Architecture"&gt;&lt;/p&gt;
&lt;p&gt;Specifically, the model had the following setup:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Layer&lt;/th&gt;
&lt;th align="center"&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Input&lt;/td&gt;
&lt;td align="center"&gt;32x32x1 Grayscale/ Normalized image&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Convolution 5x5&lt;/td&gt;
&lt;td align="center"&gt;1x1 stride, valid padding, outputs 28x28x6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;RELU activation&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Max pooling&lt;/td&gt;
&lt;td align="center"&gt;2x2 stride, valid padding, outputs 14x14x6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Convolution 5x5&lt;/td&gt;
&lt;td align="center"&gt;1x1 stride, valid padding, outputs 10x10x16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;RELU activation&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Max pooling&lt;/td&gt;
&lt;td align="center"&gt;2x2 stride, valid padding, outputs 5x5x16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Fully connected&lt;/td&gt;
&lt;td align="center"&gt;400 input, 120 output&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;RELU activation&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Dropout&lt;/td&gt;
&lt;td align="center"&gt;0.6 keep probablility (training)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Fully connected&lt;/td&gt;
&lt;td align="center"&gt;120 input, 84 output&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;RELU activation&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Dropout&lt;/td&gt;
&lt;td align="center"&gt;0.6 keep probablility (training)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Fully connected&lt;/td&gt;
&lt;td align="center"&gt;84 input, 43 output&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The above architecture with &lt;code&gt;EPOCHS = 25&lt;/code&gt;, &lt;code&gt;BATCH_SIZE = 128&lt;/code&gt;, and &lt;code&gt;LEARNING_RATE = 0.001&lt;/code&gt; ended up doing fairly well achieving:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Training Set Accuracy: ~99.8%&lt;/li&gt;
&lt;li&gt;Validation Set Accuracy: ~95%&lt;/li&gt;
&lt;li&gt;Test Set Accuracy: ~94%&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In terms of the optimizer used, I considered using various optimizers such as regular gradient descent, but one of the considerations for switching to the Adam optimizer was that the Adam optimizer takes into account "momentum". Specifically, the Adam optimizer (&lt;a href="http://sebastianruder.com/optimizing-gradient-descent/index.html#minibatchgradientdescent"&gt;read more here&lt;/a&gt;) takes "momentum" into account by keeping a decaying average of the past gradients, which usually provides quicker convergence. &lt;/p&gt;
&lt;h3&gt;Test a Model on New Images&lt;/h3&gt;
&lt;p&gt;In order to further test the model's performance on previously unseen images, I found 6 images from Google Maps Street View to test with the model. Looking at the images below, there are couple factors that might affect the model's ability to classify these new traffic signs. For example, the "Road Work" and "Road Narrows on the Right" signs are captured at an angle. This skew in the image might not get captured by the model because the training data might not have images that are similar. However, when building the training data (via augmenting more images) I tried to include rotated images to deal with this possibility. Additionally, the reduced brightness and skew in the "No Entry" image were potential problems for the classifer.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/images/sdcnd/new_internet_images.png" title="Internet Images"&gt;&lt;/p&gt;
&lt;p&gt;I plotted the top 5 predictions (based on softmax probabilities) for the 6 different images.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/images/sdcnd/softmax_prob_1.png" title="Softmax1"&gt;
&lt;img alt="alt text" src="/images/sdcnd/softmax_prob_2.png" title="Softmax2"&gt;
&lt;img alt="alt text" src="/images/sdcnd/softmax_prob_3.png" title="Softmax3"&gt;
&lt;img alt="alt text" src="/images/sdcnd/softmax_prob_4.png" title="Softmax4"&gt;
&lt;img alt="alt text" src="/images/sdcnd/softmax_prob_5.png" title="Softmax5"&gt;
&lt;img alt="alt text" src="/images/sdcnd/softmax_prob_6.png" title="Softmax6"&gt;&lt;/p&gt;
&lt;p&gt;From the results above, it seems like the concerns mentioned were actually handled fairly well by the classifier. Most of the images with the skew and brightness concerns are handled well and the probabilities also show that the model was not "unsure" between classes (except for the 80 kph vs 30 kph sign). The model achieved a ~83.3% accuracy with 5/6 images correctly classified. The 80 kph sign was misclassified as 30 kph, but this is understandable because the model might have mistaken the &lt;code&gt;8&lt;/code&gt; as a &lt;code&gt;3&lt;/code&gt;. When looking at the image, it seems quite obvious that the image is &lt;code&gt;80&lt;/code&gt;. Ultimately, I may have to add more 80 kph signs with various image translations to ensure the model does not mistakenly classify these signs.&lt;/p&gt;
&lt;p&gt;When compared to the test set, the unseen images has a lower accuracy, but it is because of the misclassification mentioned above. On the other images, the model is quite certain (high probability), while for the misclassified image has visible uncertianty (in the bar graphs of the probabilities). Overall, the model seemed to do quite well on unseen data!&lt;/p&gt;
&lt;h3&gt;Conclusions and Next Steps&lt;/h3&gt;
&lt;p&gt;This project was quite interesting and gave me the opportunity to learn more about Tensorflow, convolutional neual networks, and how to train, validate, and test a model using an Amazon EC2 instance. Additionally, I got the opportunity to experiment with preprocessing steps to improve the image classification and I learned the importance of augmenting under represented classes.&lt;/p&gt;
&lt;p&gt;While the model performed fairly well, there are improvements that can be made. &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Further regularization: Based on the training and validation curves, the model can definitely be further regularized&lt;/li&gt;
&lt;li&gt;Data augmentation: While I experimented with rotation and zooming on images, it may have helped to further translate, crop, and manipulate images to increase the training set and increase the variety of input images&lt;/li&gt;
&lt;li&gt;Hyperparameter tuning: The model performed fairly well but with more time I could have increased the number of epochs or fiddled with the batch size further to see if the model performance improves&lt;/li&gt;
&lt;li&gt;Model architecture: The LeNet architecture is a good start, but with state-of-the-art advancements such as &lt;a href="https://github.com/tensorflow/models/tree/master/inception"&gt;Google's Inception-V3&lt;/a&gt; there are other architectures that might work even better&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ultimately, this was an exciting project that gave me the opportunity to bring together many different techniques and technologies to solve and interesting problem!&lt;/p&gt;</content></entry><entry><title>Lane Finding from Video Feeds</title><link href="http://jeffwen.com/2017/02/23/lane_finding" rel="alternate"></link><published>2017-02-23T22:41:00-05:00</published><updated>2017-02-23T22:41:00-05:00</updated><author><name>Jeff Wen</name></author><id>tag:jeffwen.com,2017-02-23:/2017/02/23/lane_finding</id><summary type="html">&lt;p&gt;Color selection, region masking, canny edge detection, and hough transformation to find lane lines&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;February 23, 2017&lt;/em&gt;&lt;/p&gt;
&lt;h1&gt;Finding Lane Lines on the Road&lt;/h1&gt;
&lt;p&gt;The github repository with the associate IPython Notebook can be found &lt;a href="https://github.com/jeffwen/sdcnd_find_lanes"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Objective&lt;/h2&gt;
&lt;p&gt;The ultimate goal of this project was to be able to identify lane lines in both images and videos. Namely, to develop a pipeline that utilizes different computer vision techniques to mark the location of lane lines. The approach that I took explored multiple techniques to obtain the best results:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Color selection with OpenCV with different color maps&lt;ul&gt;
&lt;li&gt;Grayscale &lt;/li&gt;
&lt;li&gt;Hue, saturation, and light&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Gaussian blur to reduce noise&lt;/li&gt;
&lt;li&gt;Canny edge detection&lt;/li&gt;
&lt;li&gt;Hough line transformation&lt;/li&gt;
&lt;li&gt;Moving average of previous lane lines &lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Color Manipulation&lt;/h2&gt;
&lt;p&gt;To start with, images came as screenshots from an onboard video feed.&lt;/p&gt;
&lt;p&gt;&lt;img alt="lane1" src="/images/solidWhiteRight.jpg"&gt; 
&lt;img alt="lane2" src="/images/solidYellowLeft.jpg"&gt;&lt;/p&gt;
&lt;p&gt;The first step that I took was to turn the image to grayscale to make it easier to work, namely to reduce the number of channels to work with. However, when dealing with more challenging images such as lane lines that are on non-contrasting backgrounds (white or gray tarmac), the eventual pipeline for lane linea detection does not perform well. In order to improve the performance, I switched to using &lt;a href="https://en.wikipedia.org/wiki/HSL_and_HSV#HSL"&gt;hue, saturation, and light&lt;/a&gt; color space, which is better able to highlight the yellow and white lane lines.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Grayscale&lt;/em&gt;
&lt;img alt="gray" src="/images/challengeShadow_gray.jpg"&gt; &lt;/p&gt;
&lt;p&gt;&lt;em&gt;HSL color space&lt;/em&gt;
&lt;img alt="hsl" src="/images/challengeShadow_hlsimage_pyplot.jpg"&gt;&lt;/p&gt;
&lt;p&gt;In the above image, we can see that the yellow lane is very clearly highlighted and the white line markings are also captured well when compared to the grayscale image. However, to further improve the performance of the processing pipeline, we can also select out the colors that we know we care about (in this case the yellow and white lines, which are now blue and green)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;## color selection for yellow and white, using the HSL color space&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;color_selection&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;image&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="n"&gt;hls_image&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cv2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cvtColor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;image&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cv2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;COLOR_RGB2HLS&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;white_color&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cv2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inRange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hls_image&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;uint8&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;uint8&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;255&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;255&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;255&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt; &lt;span class="c1"&gt;## note that OpenCV uses BGR not RGB&lt;/span&gt;
    &lt;span class="n"&gt;yellow_color&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cv2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inRange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hls_image&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;uint8&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;uint8&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;255&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;255&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;

    &lt;span class="n"&gt;combined_color_images&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cv2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bitwise_or&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;white_color&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;yellow_color&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;cv2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bitwise_and&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;image&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;image&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mask&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;combined_color_images&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In the above code, I first convert the color map from RGB to HSL. Then I use the &lt;code&gt;inRange&lt;/code&gt; function provided by OpenCV to select colors that fall into the white and yellow ranges. After that I combine the white and yellow masks together with the &lt;code&gt;bitwise_or&lt;/code&gt; function. &lt;/p&gt;
&lt;p&gt;With the above HSL image, we can now try to isolate the yellow and the white lines. While there are many different techniques that can be utilized here, I chose to detect the edges within the image using the Canny edge detection algorithm. &lt;/p&gt;
&lt;h2&gt;Edge Detection&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;HSL color selection&lt;/em&gt;
&lt;img alt="hsl" src="/images/challengeShadow_hsl.jpg"&gt;&lt;/p&gt;
&lt;p&gt;Given the above image, the goal is to pick out the lane lines. In order to do this, I use the &lt;a href="https://en.wikipedia.org/wiki/Canny_edge_detector"&gt;canny edge detector&lt;/a&gt; algorithm. In short, the algorithm:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;1. Applies a gaussian filter to the image to reduce noise
2. Finds the gradients in both the horizontal and vertical directions
3. Non-maximum supression, which is a way to thin the detected edges by only keeping the maximum gradient values and setting others to 0
4. Determining potential edges by checking against a threshold 
5. Finish cleaning potential edges by checking in the potential edge is connected to an actual edge
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;While, the canny edge detector automatically applies &lt;a href="https://en.wikipedia.org/wiki/Gaussian_blur"&gt;gaussian blur&lt;/a&gt;, I applied gaussian blur outside of the edge detector so that I could have more freedom with the kernel parameter. After running the image through the blurring and edge detection functions, the image is as follows. Note, the input image to this is the HSL color converted image. &lt;/p&gt;
&lt;p&gt;&lt;em&gt;HSL color selection with canny edge detection&lt;/em&gt;
&lt;img alt="hsl_canny" src="/images/challengeShadow_hslcanny.jpg"&gt;&lt;/p&gt;
&lt;p&gt;With the image above, we see that the lane lines are pretty well identified. It took a bit of trial and error to find suitable thresholds for the canny edge detector though the creator John Canny recommended a ratio of 1:2 or 1:3 for the low vs. high threshold. Although the image above seems to mark the lane lines quite well, there is still a lot of noise surrounding the lane that we do not care about. In order to address this, we can apply a region mask to just keep the area that we know contains the lane lines. &lt;/p&gt;
&lt;p&gt;&lt;em&gt;Region masking&lt;/em&gt;
&lt;img alt="region_bounds" src="/images/challengeShadow_regionmask.jpg"&gt;&lt;/p&gt;
&lt;p&gt;After applying the mask to the canny image, we get the following output. We can contrast this with the gray image after canny edge detection and the region selection. &lt;/p&gt;
&lt;p&gt;&lt;em&gt;Grayscale image with canny edge detection and region masking&lt;/em&gt;
&lt;img alt="region_canny_gray" src="/images/challengeShadow_grayregioncanny.jpg"&gt; &lt;/p&gt;
&lt;p&gt;&lt;em&gt;HSL color selection with canny edge detection and region masking&lt;/em&gt;
&lt;img alt="region_canny" src="/images/challengeShadow_regioncanny.jpg"&gt; &lt;/p&gt;
&lt;p&gt;As shown above, the HSL version provides a cleaner indication of the lane lines. Below are the functions used in processing the images.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;canny&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;low_threshold&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;high_threshold&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Applies the Canny transform&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;cv2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Canny&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;low_threshold&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;high_threshold&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;gaussian_blur&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Applies a Gaussian Noise kernel&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;cv2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GaussianBlur&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;region_of_interest&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;vertices&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="sd"&gt;    Applies an image mask.&lt;/span&gt;

&lt;span class="sd"&gt;    Only keeps the region of the image defined by the polygon&lt;/span&gt;
&lt;span class="sd"&gt;    formed from `vertices`. The rest of the image is set to black.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="c1"&gt;#defining a blank mask to start with&lt;/span&gt;
    &lt;span class="n"&gt;mask&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros_like&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;   

    &lt;span class="c1"&gt;#defining a 3 channel or 1 channel color to fill the mask with depending on the input image&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;channel_count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;  &lt;span class="c1"&gt;# i.e. 3 or 4 depending on your image&lt;/span&gt;
        &lt;span class="n"&gt;ignore_mask_color&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;255&lt;/span&gt;&lt;span class="p"&gt;,)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;channel_count&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;ignore_mask_color&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;255&lt;/span&gt;

    &lt;span class="c1"&gt;#filling pixels inside the polygon defined by &amp;quot;vertices&amp;quot; with the fill color    &lt;/span&gt;
    &lt;span class="n"&gt;cv2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fillPoly&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mask&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;vertices&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ignore_mask_color&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;#returning the image only where mask pixels are nonzero&lt;/span&gt;
    &lt;span class="n"&gt;masked_image&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cv2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bitwise_and&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mask&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;masked_image&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Hough Line Transform&lt;/h2&gt;
&lt;p&gt;Now that we have a collection of edges, we need to identify the lane lines within the image. The &lt;a href="https://en.wikipedia.org/wiki/Hough_transform"&gt;hough line transform&lt;/a&gt;, which was first invented to identify lines within images, is great for this task. To learn more about this algorithm, this &lt;a href="http://alyssaq.github.io/2014/understanding-hough-transform/"&gt;blog&lt;/a&gt; is a great resource. &lt;/p&gt;
&lt;p&gt;_HSL color selection with canny edge detection, region masking, and hough transform
&lt;img alt="hsl_hough" src="/images/challengeShadow_hlshoughimage_pyplot.jpg"&gt;&lt;/p&gt;
&lt;p&gt;Pretty awesome! The lane lines have now been highlighted and boxed with the red lines. There are quite a few parameters that needed to be adjusted, but after adjusting the parameters, the algorithm is able to pick out the lines quite well. Note that the OpenCV version of the hough transform that we are using is the probabilistic version, which is an improvement over the original. In the IPython notebook, I use a different version of the &lt;code&gt;hough_lines&lt;/code&gt; function that simple outputs the lines as a vector rather than overlaying the lines over the initial image. &lt;/p&gt;
&lt;p&gt;Given the above image and specifically the hough lines, we now have a vector of multiple lines segments in the form of (x1,y1,x2,y2) endpoints. In order to draw lines on an image, we need a way to extrapolate an average line from the vector of endpoints.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;hough_lines_overlay&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;rho&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;threshold&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;min_line_len&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_line_gap&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="sd"&gt;    `img` should be the output of a Canny transform.&lt;/span&gt;

&lt;span class="sd"&gt;    Returns an image with hough lines drawn.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;lines&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cv2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;HoughLinesP&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;rho&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;threshold&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([]),&lt;/span&gt; &lt;span class="n"&gt;minLineLength&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;min_line_len&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;maxLineGap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;max_line_gap&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;line_img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;uint8&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;draw_lines&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;line_img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lines&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;line_img&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Lane Line Averaging&lt;/h2&gt;
&lt;p&gt;In order to find the average line on each side of the lane, we can first calculate the slope of each line segment and separate the positive and negative sloped lines, which represents the left and right lane lines. Then we can find the average of the left and right slopes and intercepts to get an average of the lanes. When I initially did this, the average was quite sensitive to outliers. I tried to adjust for the outliers by removing points that were greater than 1.5 standard deviations from the rest of the slopes. However, the averaged lines were still quite sensitive to the outliers. &lt;/p&gt;
&lt;p&gt;Ultimately, by calculating the line length and calculating the weighted average of the lane line, the output was much more stable and robust against spurious line segments that the hough transform identified. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;avg_lines&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lines&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="n"&gt;neg&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;empty&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;pos&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;empty&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

    &lt;span class="c1"&gt;## calculate slopes for each line to identify the positive and negative lines&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;lines&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;x2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y2&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;slope&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y2&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;y1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x2&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;x1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;intercept&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;slope&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;x1&lt;/span&gt;
            &lt;span class="n"&gt;line_length&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;y2&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;y1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x2&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;x1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;slope&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="n"&gt;line_length&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;neg&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;neg&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="n"&gt;slope&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;intercept&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;line_length&lt;/span&gt;&lt;span class="p"&gt;]]),&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="n"&gt;slope&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="n"&gt;line_length&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;pos&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pos&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="n"&gt;slope&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;intercept&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;line_length&lt;/span&gt;&lt;span class="p"&gt;]]),&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;## just keep the observations with slopes with 2 std dev&lt;/span&gt;
    &lt;span class="n"&gt;neg&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;neg&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;to_keep_index&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;neg&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])]&lt;/span&gt;
    &lt;span class="n"&gt;pos&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pos&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;to_keep_index&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pos&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])]&lt;/span&gt;

    &lt;span class="c1"&gt;## weighted average of the slopes and intercepts based on the length of the line segment&lt;/span&gt;
    &lt;span class="n"&gt;neg_lines&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;neg&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;neg&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:,:&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;neg&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;neg&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
    &lt;span class="n"&gt;pos_lines&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pos&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;pos&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:,:&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pos&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pos&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;neg_lines&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pos_lines&lt;/span&gt;

&lt;span class="c1"&gt;## removing the outliers (adapted from http://stackoverflow.com/questions/11686720/is-there-a-numpy-builtin-to-reject-outliers-from-a-list)&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;to_keep_index&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;obs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.5&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;obs&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;obs&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;obs&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The above function calculates the slope, intercept, and line length of each line segment. At this point, we can take the average lane lines from the above function and plot the lane lines onto the original image. &lt;/p&gt;
&lt;p&gt;&lt;em&gt;Final processed image&lt;/em&gt;
&lt;img alt="hsl_final" src="/images/challengeShadow_processed.jpg"&gt;&lt;/p&gt;
&lt;p&gt;It seems to have performed quite well! Below are a few other sample images of the outputs from the lane finding pipeline. &lt;/p&gt;
&lt;p&gt;&lt;em&gt;Sample processed images&lt;/em&gt;
&lt;img alt="solid_yellow" src="/images/solidYellowCurve_processed.jpg"&gt; 
&lt;img alt="solid_white" src="/images/solidWhiteRight_processed.jpg"&gt;&lt;/p&gt;
&lt;h2&gt;Applying Lane Finding to Videos&lt;/h2&gt;
&lt;p&gt;Now that we can identify and mark the lane lines within the image supplied, we can use the algorithm on a video, which is just a sequence of images. If we just apply the pipeline directly to the video, we get the following. &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://vimeo.com/205495473"&gt;Lane Finding (Without Previous Averaging)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The video seems to show the lane lines without any problems, but when we take a closer look the lane line highlights are jittering and jumping across back and forth around the actual location of the lane line. While, the algorithm basically accomplishes the problem that we first set out to solve, maybe we can improve on this. &lt;/p&gt;
&lt;p&gt;Specifically, the lane lines coming from a video feed usually do not change dramatically from second to second. If we take this into account, we can "smooth" the lane lines plotted out by keeping a queue. With each frame of the video, we can pop off the oldest set of lane line endpoints. Then for all the remaining lane lines and the newest lane line, we take an average to get the "smoothed" lane line. &lt;/p&gt;
&lt;p&gt;Below is the code for the lane line detector and the link to the test videos.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://vimeo.com/205495845"&gt;Lane Finding - White Line (With Averaging)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://vimeo.com/205495681"&gt;Lane Finding - Yellow Line (With Averaging)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://vimeo.com/205495856"&gt;Lane Finding - Curve (With Averaging)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The above videos show that the new detector with the lane line averaging works quite nicely! Although if there are drastic changes the algorithm does not follow those changes until a bit later, we can fiddle with this by changing the size of the queue.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;lane_detector&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;prev_lane_lines&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;find_mean_lines&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lane_lines&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;prev_lane_lines&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

        &lt;span class="c1"&gt;## add the new lane line&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;lane_lines&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;prev_lane_lines&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lane_lines&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;## only keep the 10 most recent lane lines&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prev_lane_lines&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;prev_lane_lines&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;## take the average of the past lane lines and the new ones&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prev_lane_lines&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prev_lane_lines&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;pipeline&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;image&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;imshape&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;image&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;

        &lt;span class="c1"&gt;## selecting yellow and white colors&lt;/span&gt;
        &lt;span class="n"&gt;color_selected_img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;color_selection&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;image&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;## Define a kernel size and apply Gaussian smoothing&lt;/span&gt;
        &lt;span class="n"&gt;kernel_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;17&lt;/span&gt;
        &lt;span class="n"&gt;blur_img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cv2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GaussianBlur&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;color_selected_img&lt;/span&gt;&lt;span class="p"&gt;,(&lt;/span&gt;&lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;## apply canny edge detection&lt;/span&gt;
        &lt;span class="n"&gt;canny_img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;canny&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;blur_img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;low_threshold&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;high_threshold&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;160&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;## apply region of interest&lt;/span&gt;
        &lt;span class="n"&gt;vertices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;imshape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]),(&lt;/span&gt;&lt;span class="n"&gt;imshape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;*.&lt;/span&gt;&lt;span class="mi"&gt;45&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;imshape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mf"&gt;0.6&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;imshape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;*.&lt;/span&gt;&lt;span class="mi"&gt;55&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;imshape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mf"&gt;0.6&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;imshape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;imshape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])]],&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;region_img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;region_of_interest&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;canny_img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;vertices&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;vertices&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;## apply hough transformation&lt;/span&gt;
        &lt;span class="n"&gt;rho&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="c1"&gt;# distance resolution in pixels of the Hough grid&lt;/span&gt;
        &lt;span class="n"&gt;theta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pi&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;180&lt;/span&gt; &lt;span class="c1"&gt;# angular resolution in radians of the Hough grid&lt;/span&gt;
        &lt;span class="n"&gt;threshold&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt; &lt;span class="c1"&gt;# minimum number of votes (intersections in Hough grid cell)&lt;/span&gt;
        &lt;span class="n"&gt;min_line_len&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;25&lt;/span&gt; &lt;span class="c1"&gt;#minimum number of pixels making up a line&lt;/span&gt;
        &lt;span class="n"&gt;max_line_gap&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;250&lt;/span&gt;   &lt;span class="c1"&gt;# maximum gap in pixels between connectable line segments&lt;/span&gt;

        &lt;span class="n"&gt;lines&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;hough_lines&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;region_img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;rho&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;threshold&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;min_line_len&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_line_gap&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;## get the average slopes and intercepts for each lane line&lt;/span&gt;
        &lt;span class="n"&gt;slopes_intercepts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;avg_lines&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lines&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# find the endpoints given the slopes and intercepts&lt;/span&gt;
        &lt;span class="n"&gt;endpoints&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gen_endpoints&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;image&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;slopes_intercepts&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;## generate lane lines on a black image&lt;/span&gt;
        &lt;span class="n"&gt;lane_lines&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gen_lane_lines&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;image&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;endpoints&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;find_mean_lines&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;endpoints&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;prev_lane_lines&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

        &lt;span class="n"&gt;final_img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;weighted_img&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lane_lines&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;image&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;final_img&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Shortcomings &amp;amp; Next Steps&lt;/h2&gt;
&lt;p&gt;While the detector works fairly well for straight roads, there are limitations:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Curved Roads&lt;/li&gt;
&lt;li&gt;Lane markings that are not yellow or white&lt;/li&gt;
&lt;li&gt;Different perspective &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In order to deal with these shortcomings, we would need to make the algorithm more robust to differences in the input video. For example, to deal with the curves in the road instead of setting a fixed length for the lane line highlights, which is currently 60% of the image height, we might be able to use the length of the identified line segment from th hough line transform as a proxy for how long the highlight should be. &lt;/p&gt;
&lt;p&gt;The yellow and white lane lines might be harder to deal with, but we can combine human input as well as computational methods. For example, if there are training images from roads in different areas with different colored markings, we can keep a "dictionary" of these lane colors and setup the algorithm to look for the colors that expected given the geographic region in consideration. &lt;/p&gt;
&lt;p&gt;The videos that were supplied as test videos were all basically filmed at the same angle and the roads were also fairly similar. However, if the vehicle was traveling over a hill or out of a trough the perspective would change. In these cases, the algorithm might not perform as well. In order to adjust for this, we can first apply a perspective normalization to the input video so that input would always have the same orientation and perspective. &lt;/p&gt;
&lt;p&gt;Overall, this project was interesting and fun! It incorporated a lot of techniques and concepts that have been available for many years, but is now being applied to interesting problems like self-driving cars. &lt;/p&gt;</content></entry><entry><title>Visaurant: Reimagining the food search experience</title><link href="http://jeffwen.com/2016/05/02/visaurant" rel="alternate"></link><published>2016-05-02T14:26:00-04:00</published><updated>2016-05-02T14:26:00-04:00</updated><author><name>Jeff Wen</name></author><id>tag:jeffwen.com,2016-05-02:/2016/05/02/visaurant</id><summary type="html">&lt;p&gt;Using computer vision techniques to visually filter and search through images&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;May 02, 2016&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Visaurant Logo" src="/images/visaurant_logo.png"&gt;
This blog post is a work in progress and will detail the process that I took to create the Visaurant project.&lt;/p&gt;
&lt;p&gt;Visaurant (as you can see in the video below) is a reimagination of the way users search through images that they are interested in. One prime use case for Visaurant is in sorting and filtering through food images (hence &lt;em&gt;VIS&lt;/em&gt; -ual rest- &lt;em&gt;AURANT&lt;/em&gt;).&lt;/p&gt;
&lt;h3&gt;Background&lt;/h3&gt;
&lt;p&gt;So why do we want a visual search application?&lt;/p&gt;
&lt;p&gt;Images represent a wealth of information. As cliche as it sounds, &lt;em&gt;'a picture is worth a thousand words'&lt;/em&gt; especially when it comes to looking for food items that seem appetizing. Speaking from personal experience, I often use apps like Yelp by:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Going to the images of a particular restaurant&lt;/li&gt;
&lt;li&gt;Looking for the pictures that look good&lt;/li&gt;
&lt;li&gt;Reading the caption&lt;/li&gt;
&lt;li&gt;Going back to the reviews to find the reviews regarding the item that I saw&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;While this process works, it is by no means streamlined. Having to go back and forth between the images, captions, and reviews is a hassle and quite often overwhelming because there are so many images to sift through. So although images are powerful and contain a lot of information, if the images are not ordered or organized then it becomes difficult to find what I care about (like the example below...).&lt;/p&gt;
&lt;p&gt;&lt;img alt="Yelp Restaurant Photos" src="/images/yelp_photo.png"&gt;&lt;/p&gt;
&lt;p&gt;Visaurant aims to change that by:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Clustering visually similar images to allow users to quickly narrow down to images of interest&lt;/li&gt;
&lt;li&gt;Parsing through captions to extract reviews that are relevant to the image that was selected&lt;/li&gt;
&lt;/ol&gt;
&lt;iframe src="https://player.vimeo.com/video/165064797" width="640" height="344" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;

&lt;h4&gt;What is happening in the video?&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;User gets to select a group of images from a restaurant that looks interesting (4 selections)
&lt;img alt="Visaurant Selection" src="/images/visaurant_selection.png"&gt;&lt;/li&gt;
&lt;li&gt;Recommendations will be made based on the user's selections&lt;/li&gt;
&lt;li&gt;User can once again select the restaurant that looks most appealing
&lt;img alt="Visaurant Recommendation" src="/images/visaurant_recommendation.png"&gt;&lt;/li&gt;
&lt;li&gt;Restaurant specific page will show up allowing the user to hover over images that look interesting and images within the same visual cluster will be highlighted
&lt;img alt="Visaurant Highlight" src="/images/visaurant_highlight.png"&gt;&lt;/li&gt;
&lt;li&gt;User can select an image and all images will be filtered for images in the same cluster
&lt;img alt="Visaurant Filtered" src="/images/visaurant_filtered.png"&gt;&lt;/li&gt;
&lt;li&gt;After filtering down to a single cluster, user can select a specific image to bring up reviews that have matching words with the image's caption
&lt;img alt="Visaurant Caption" src="/images/visaurant_caption.png"&gt;&lt;/li&gt;
&lt;/ol&gt;</content></entry><entry><title>Mr. President, what did you say?</title><link href="http://jeffwen.com/2016/03/18/mr_president_what_did_you_say" rel="alternate"></link><published>2016-03-18T22:18:00-04:00</published><updated>2016-03-18T22:18:00-04:00</updated><author><name>Jeff Wen</name></author><id>tag:jeffwen.com,2016-03-18:/2016/03/18/mr_president_what_did_you_say</id><summary type="html">&lt;p&gt;Natural language processing, latent semantic indexing, latent dirichlet allocation, and D3.js visualizations!&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;March 18, 2016&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;I know this may be annoying but embedding D3 in Pelican isn't the easiest thing so PLEASE click &lt;a href="/html/president.html"&gt;here&lt;/a&gt; to see the actual D3! The rest of the blog here will detail the process.&lt;/p&gt;
&lt;p&gt;Also, yes very soon I will actually deploy these things somewhere so they aren't just static HTML pages...&lt;/p&gt;
&lt;p&gt;But still, hope you enjoy!&lt;/p&gt;
&lt;p&gt;&lt;a href="/html/president.html"&gt;&lt;img alt="President Network" src="/images/president_network.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;What are we doing here&lt;/h3&gt;
&lt;p&gt;I would like to provide some details both for whoever is reading and also for myself so I remember this project.&lt;/p&gt;
&lt;p&gt;This particular project was one of the most difficult to date for me because it required the use of a couple different technologies in unison and also with this project, I tried to actually build most of the D3 visualizations by myself. As a result, it took a bit of time and I am quite proud of the outcome!&lt;/p&gt;
&lt;p&gt;The objectives of this project:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Use MongoDB to store some semi-structured data (namely text data)&lt;/li&gt;
&lt;li&gt;Perform some form of natural language processing over the collected data&lt;/li&gt;
&lt;li&gt;Present the results in an interesting way&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The broader goal was to see if I could analyze text and see how presidents' were either similar or different in the topics that they discussed. I also wanted to identify if those topics changed across time.&lt;/p&gt;
&lt;h3&gt;MongoDB&lt;/h3&gt;
&lt;p&gt;MongoDB is really easy to get running. Remember that post about the &lt;a href="/2016/02/27/making_census_data_exciting_part_1"&gt;census data&lt;/a&gt;? Well let me just post a little bit of code to show how quick it is to set up MongoDB (granted, this is on my local machine and not on AWS...but still I'm trying to make a point so forgive me).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# install with homebrew (mac)&lt;/span&gt;
brew install mongodb
&lt;span class="c1"&gt;# make a data directory (for mongo to write to)&lt;/span&gt;
sudo mkdir /data/db
&lt;span class="c1"&gt;# change ownership for the directory and start the mongo server&lt;/span&gt;
sudo chown your_username /data/db
mongod
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Boom. After this you can just &lt;code&gt;mongoimport&lt;/code&gt; entire &lt;code&gt;.json&lt;/code&gt; files into your database as different collections or however you'd like. So all of that to say that it is definitely easier to get things set up and data into the database (not to say that SQL isn't good...they are good for different things).&lt;/p&gt;
&lt;p&gt;With the database set up, I had to get my data.&lt;/p&gt;
&lt;h3&gt;Presidential Data&lt;/h3&gt;
&lt;p&gt;There were a couple sites that had some great resources, but the one that I ended up using was the UC Santa Barbara &lt;a href="http://www.presidency.ucsb.edu/sou.php"&gt;American Presidency Project&lt;/a&gt;. It has speeches, press releases, and much more dating back to President Washington.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.presidency.ucsb.edu/sou.php"&gt;&lt;img alt="American Presidency Project" src="/images/presidency_project.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;While it would have been great to have some form of an API to make calls to and get nicely formatted data, I did not have that luxury in this case and so I scraped the data that I needed using BeautifulSoup. Usually, this wouldn't be the easiest part, but this time it was actually quite simple!&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# make the request to the website&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;requester&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;    url: url of the site to be scraped&lt;/span&gt;
&lt;span class="sd"&gt;    return: BeautifulSoup obejct of the scraped text&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span class="n"&gt;response&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;requests&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;response&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ok&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;BeautifulSoup&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;response&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# state of the union addresses urls&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_union_address_url&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;requester&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# only extract the links from the page that link to speeches&lt;/span&gt;
    &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;find&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;table&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;find_all&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;href&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;.*/ws/index&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;speech_urls&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;link&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;speech_urls&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;link&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;href&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;speech_urls&lt;/span&gt;

&lt;span class="c1"&gt;# extract the text from the speeches as a dictionary&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;parse_union_address&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;requester&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;try&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;president&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;title&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;find&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;title&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;:&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;except&lt;/span&gt; &lt;span class="ne"&gt;ValueError&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;president&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
        &lt;span class="n"&gt;title&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;

    &lt;span class="c1"&gt;# create a dictionary with the details of interest&lt;/span&gt;
    &lt;span class="n"&gt;date&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strptime&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;find&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;span&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;class&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;docdate&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;%B &lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;, %Y&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;text&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;find&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;span&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;class&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;displaytext&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;

    &lt;span class="c1"&gt;# each paragraph is separated into (as it will be considered a document in later analyses)&lt;/span&gt;
    &lt;span class="n"&gt;text_list&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;find_all&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;text_list&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;text_list&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;aDict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;president&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;president&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;title&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;date&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;date&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;text&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;text_list&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;url&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;speech&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;State of the Union&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;aDict&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Those three functions above are a sample of the functions that I wrote to scrape the State of the Union Addresses from the website. I had similar functions to scrape Inaugural Addresses and press releases. After getting all the text, I created dictionaries with the details of the speeches and used &lt;code&gt;pymongo&lt;/code&gt; to connect to mongoDB so that I could store the data.&lt;/p&gt;
&lt;p&gt;It wasn't necessary to use mongoDB because I used Python to scrape the data and was going to manipulate the data in Python,but I wanted to learn how to use the database for future reference. Furthermore, in case anything happened I wouldn't have to rely on the pickled files I created (as an extra saftey measure...).&lt;/p&gt;
&lt;h3&gt;Network Graph&lt;/h3&gt;
&lt;p&gt;If you took a look at the HTML page that I linked to at the beginning with the network graph of the presidents, you'll notice that there are clusters of presidents. Initially, I hypothesized that the presidents would be clustered by their respective parties; however, I turned out to be really wrong! They were mostly clustered based by the period in which they were president. This makes sense and I will discuss this later.&lt;/p&gt;
&lt;p&gt;So how did I generate those clusters? Well, let me start off by talking about how I got to clustering. &lt;/p&gt;
&lt;h4&gt;Latent Semantic Indexing&lt;/h4&gt;
&lt;p&gt;One of the interesting techniques that I wanted to use was &lt;a href="https://en.wikipedia.org/wiki/Latent_semantic_indexing"&gt;latent semantic indexing&lt;/a&gt;. Very basically, this technique uses matrix factorization (singular value decomposition) to map the documents into a vector space, then we specify the rank (kind of like the number of concepts that we want to keep). Afterwards, the passed in term frequency matrix is factorized into a term concept matrix, a square singular value matrix, and a concept document matrix. The concept document matrix can be thought of as a matrix that captures the latent 'concepts' within the documents.&lt;/p&gt;
&lt;p&gt;I used the concept document matrix and computed the cosine similarity between each of the different vectors (in this case each president had his own vector that signified the concepts in his speeches and press releases). Then I created the connections by simply setting the threshold to &amp;gt;= 0.75 (I experimented with this for a bit to see how the connections would turn out; a bit arbitrary, but if I had more time I would have kept all connections and used the weight/width of the connection to represent the cosine similarity).&lt;/p&gt;
&lt;h3&gt;Taking a Step Back&lt;/h3&gt;
&lt;p&gt;With the network created, I realized that the connections were actually based more on the time period in which the presidents served. So I added the color and labels to identify the presidents by the period in which they served. I also included the party toggle to show that my initial hypothesis was incorrect.&lt;/p&gt;
&lt;p&gt;In order to learn more about what the presidents' talked about, I decided to dive down into the particular time periods. My thought was that if I could limit the president set to within the same time period, then some of the finer details of topics discussed would show up.&lt;/p&gt;
&lt;h4&gt;Latent Dirichlet Allocation&lt;/h4&gt;
&lt;p&gt;&lt;a href="/html/president.html"&gt;&lt;img alt="LDA Topics" src="/images/lda_bubbles.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In order to extract the topics that were discussed in the different speeches and press releases, I decided that it would be more accurate to run the analyses on separate paragraphs. When a president gives a State of the Union Address or Inaugural Address, each paragraph is typically a separate topic; therefore, I made each paragraph an individual document (if it was more than 10 words long).&lt;/p&gt;
&lt;p&gt;I used &lt;a href="http://jmlr.csail.mit.edu/papers/v3/blei03a.html"&gt;latent dirichlet allocation&lt;/a&gt;, which is a probabilistic approach to generating topics. Very basically, we start by randomly assigning k topics to a document (a mixture of topics), then each word in the document to a topic. This is a fairly poor representation of the topics at this stage, but it improves by going through and updating the topic for each word by choosing a new topic with probability that the new topic generated this word (here is an &lt;a href="https://www.quora.com/What-is-a-good-explanation-of-Latent-Dirichlet-Allocation/answer/Edwin-Chen-1?srid=CiUY"&gt;awesome explanation&lt;/a&gt; that is a lot more clear! Go read! seriously!).&lt;/p&gt;
&lt;p&gt;Therefore, for each paragraph I extracted the most likely topic words (from the LDA output). Just to finish things up, I connected these topic words back to the initial paragraphs so that I could see the paragraphs that LDA determined to be a certain topic.&lt;/p&gt;
&lt;h3&gt;Summary&lt;/h3&gt;
&lt;p&gt;This project took quite a bit of coordination and also learning because I was trying to use different technologies that I didn't have much experience with. However, it was one of the most fulfilling projects, because by the end I had something that not only summarized the analyses that I performed, but also an interactive demonstration so that others could explore as well.&lt;/p&gt;
&lt;p&gt;With that said, there are definitely improvements to be made.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;It isn't long before you notice that some of the initial paragraphs in the scrolling text box are not paragraphs. Instead, there are headers, questions, and other irrelevant forms of text. These should be taken out because they do not really represent paragraphs that the presidents spoke&lt;/li&gt;
&lt;li&gt;As an additional analyses, it would be interesting to run LDA on speeches that aren't State of the Union or Inaugural Addresses because these speeches are typically very broad ranging in topics discussed and therefore each president in the same period of time would most likely be discussing similar matters (think Obama and Bush talking about war...)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Anyways, those are just a few that came to mind. I hope you enjoyed the post and the visualizations!&lt;/p&gt;</content></entry><entry><title>Making Census Data Exciting (Part 2)</title><link href="http://jeffwen.com/2016/03/18/making_census_data_exciting_part_2" rel="alternate"></link><published>2016-03-18T17:47:00-04:00</published><updated>2016-03-18T17:47:00-04:00</updated><author><name>Jeff Wen</name></author><id>tag:jeffwen.com,2016-03-18:/2016/03/18/making_census_data_exciting_part_2</id><summary type="html">&lt;p&gt;Getting data out of a remote server and show results on a D3 dashboard&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;March 18, 2016&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In this post, I discuss the results of the work from the &lt;a href="/2016/02/27/making_census_data_exciting_part_1"&gt;first post&lt;/a&gt;. That first post was a bit lengthy and went into the details of getting the PostgreSQL set up, but that was all necessary to be able to query the data to set up the &lt;a href="/html/dashboard.html"&gt;dashboard&lt;/a&gt; (a screenshot is shown below, you can click on multiple things at once...cross filter charts)!&lt;/p&gt;
&lt;p&gt;&lt;a href="/html/dashboard.html"&gt;&lt;img alt="Dashboard" src="/images/dashboard.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Problem Statement&lt;/h3&gt;
&lt;p&gt;So the goal of the project was to see if &lt;a href="https://archive.ics.uci.edu/ml/datasets/Census+Income"&gt;census data&lt;/a&gt; could be used in an interesting manner. While discussing possible problems that could be solved, our team figured that it would be interesting to use the data as if we had just acquired a freelance employment company. Let me make this more clear:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The parent company (a job posting company, think Monster.com), had just acquired a freelance job posting company (they have many freelance, gig type job postings)&lt;/li&gt;
&lt;li&gt;The parent company has a database of existing users but because it was not focused on freelance and gig type jobs, the database does not have hours worked per week as a feature for its users&lt;/li&gt;
&lt;li&gt;Machine learning would be used to predict how many hours an individual works (we bucketed this into &amp;lt; 40 hours or &amp;gt;= 40 hours)&lt;/li&gt;
&lt;li&gt;Ultimately, in the hypothetical world, we would then target individuals that (we predicted) worked &amp;lt; 40 hours with the part-time job postings (from the acquired company)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;After the prediction, was the visualization aspect that would help our hypothetical company's managers to slice and dice data as they wanted.&lt;/p&gt;
&lt;h3&gt;Approach&lt;/h3&gt;
&lt;p&gt;As part of the problem, I wanted to use PostgreSQL to get used to using it in a remote environment so that I could learn to store my data remotely (hence the previous post). However, the next step was to get the data out of the database so that it could be manipulated and massaged for further analysis.&lt;/p&gt;
&lt;p&gt;For this, Psycopg2 was the best tool as it allowed for a pretty straight forward way to connect with the remote database. I wrote a Python script so that anyone on my team could quickly enter their login details and the script would ask for the SQL query. Then, the output would be a pandas dataframe of the data that was requested.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# input must be strings&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;query_database&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;user&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dbname&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;password&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;host&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;port&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;    dbname: database name&lt;/span&gt;
&lt;span class="sd"&gt;    user: username&lt;/span&gt;
&lt;span class="sd"&gt;    password: password for the user&lt;/span&gt;
&lt;span class="sd"&gt;    host: public dns&lt;/span&gt;
&lt;span class="sd"&gt;    port: typically for postgresql 5432&lt;/span&gt;
&lt;span class="sd"&gt;    returns a pandas dataframe of the given query&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;

    &lt;span class="k"&gt;try&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="c1"&gt;# Create connection with database&lt;/span&gt;
        &lt;span class="n"&gt;conn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;psycopg2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;connect&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;dbname=&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;dbname&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot; user=&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;user&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot; password=&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;password&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot; host=&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;host&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot; port=&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;port&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Connected&amp;quot;&lt;/span&gt;
        &lt;span class="n"&gt;cur&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;conn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cursor&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

        &lt;span class="c1"&gt;# Ask for user&amp;#39;s SQL query&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Query please: &amp;quot;&lt;/span&gt;
        &lt;span class="n"&gt;input_query&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;raw_input&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

        &lt;span class="c1"&gt;# Execute search query&lt;/span&gt;
        &lt;span class="n"&gt;cur&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;execute&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_query&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cur&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fetchall&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

        &lt;span class="c1"&gt;# Return dataframe&lt;/span&gt;
        &lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;cur&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;close&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;conn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;close&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;

    &lt;span class="k"&gt;except&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Connection error or query mistake&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This made it quite easy to get the data out of the AWS database. The next steps was to clean the data and then run models to predict the hours worked.&lt;/p&gt;
&lt;p&gt;In terms of cleaning the data, I wrote a function that took as input the previously outputted dataframe and returned a cleaned dataframe with white spaces removed from column headers, hours per week worked turned into a binary variable (&amp;lt; 40 hours worked or &amp;gt;= 40 hours worked), and other categorical variables turned into dummy variable columns. &lt;/p&gt;
&lt;h3&gt;Predictions&lt;/h3&gt;
&lt;p&gt;For the next step, we used the cleaned data as input to a couple different classification models. In order to test our models' accuracy we used 3 fold cross validation and achieved an accuracy score of ~70%. We could have used other error metrics but in our case we felt like the model was performing as good as we would have expected (we actually averaged couple models to achieve this accuracy).&lt;/p&gt;
&lt;p&gt;Given the business problem, we were okay with 70% because for our hypothetical company, any classification of &amp;lt; 40 hours worked (part-time) would mean that those individuals now had a lot of new job postings to browse (we acquired a freelance/ gig job posting company). Therefore, any exposure to these job postings would be better than nothing at all. Of course, in reality as we begin to get responses to the part-time job postings we would refine the prediction to make sure that we aren't showing part-time jobs to individuals who are actually working full-time (false positives).&lt;/p&gt;
&lt;h3&gt;Dashboard&lt;/h3&gt;
&lt;p&gt;The dashboard. Oh man. Okay, so a big part of this project was to practice using D3. Honestly, I had seen D3 used before, but I had no idea how flexible it was and how cool it was (the ease of use is something else). So I was quite excited to get my hands dirty.&lt;/p&gt;
&lt;p&gt;The great thing about making dashboards is that there are lots of examples to draw from. For this particular dashboard, I used an example that &lt;a href="http://bl.ocks.org/saraquigley/81807cb241cb4bbbaa6b"&gt;Sara Quigley&lt;/a&gt; had made.&lt;/p&gt;
&lt;p&gt;By the end, the resulting dashboard was quite different and I felt like I got to experiment with quite a lot of different techniques. &lt;em&gt;(as an aside: I am thankful for templates and also think that if possible you should make use of resources that are available, but make sure you know what is actually happening when you change things!)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;I spent a lot of time reading through the code to try to understand how cross filter objects were being created and used. By the end, I had taken out a couple charts and also added a few items to the dashboard so that it fit out business problem&lt;/p&gt;
&lt;p&gt;Also reading &lt;a href="https://github.com/mbostock/d3/wiki/API-Reference"&gt;documentation&lt;/a&gt; is probably one of the best ways to learn so I did a lot of that.&lt;/p&gt;
&lt;h3&gt;Summary&lt;/h3&gt;
&lt;p&gt;Overall, this project was quite interesting in that it had multiple moving parts that came together as a single dashboard deliverable. The awesome thing about this project was that it was pretty open ended so the problem statement that we came up with actually seemed like a real problem.&lt;/p&gt;
&lt;p&gt;I spent most of my time on setting up the database, writing the script to pull data out, and tweaking/customizing the dashboard. In terms of takeaways, I realized that visualization is a large aspect of data science because the communication of results is vital to any problem. More specifically, a visually appealing tool can really help tell an impactful story (of course, the tool has to make sense).&lt;/p&gt;
&lt;p&gt;Working with PostgreSQL and D3 helped me understand a more complete picture of the analytics space than just using Python. Moving forward, I would like to continue expanding my knowledge in these technologies and more!&lt;/p&gt;</content></entry><entry><title>Making Census Data Exciting (Part 1)</title><link href="http://jeffwen.com/2016/02/27/making_census_data_exciting_part_1" rel="alternate"></link><published>2016-02-27T13:31:00-05:00</published><updated>2016-02-27T13:31:00-05:00</updated><author><name>Jeff Wen</name></author><id>tag:jeffwen.com,2016-02-27:/2016/02/27/making_census_data_exciting_part_1</id><summary type="html">&lt;p&gt;Setting up the AWS EC2 used to store census data and also discussing some project design choices&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;February 27, 2016&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Let's talk about remote servers, census data, classification models, and data visualization...lots to cover...&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Warning:&lt;/em&gt; This post will be a bit lengthy and cover some details that might not be that interesting but I am also using this post as a way to capture the process so I don't forget things.
&lt;img alt="Census Logo" src="/images/census-logo.png"&gt;&lt;/p&gt;
&lt;h3&gt;Summary&lt;/h3&gt;
&lt;p&gt;In this post, I will discuss some of the design decisions that I made while working on this project and also go through some of the set up required to get things up and running. To start off with the goal of this project was to take census data and use it and visualize it in an interesting way.&lt;/p&gt;
&lt;p&gt;While I did not really have a clear idea when I started the project, I worked with a couple of my teammates to come up with an overall plan. We had to use &lt;a href="https://archive.ics.uci.edu/ml/datasets/Census+Income"&gt;census data&lt;/a&gt; and also had to think about how to tell a story with that data using classification models and D3.js. With this said, we really racked our brains to try to figure out how to use the data in an interesting way. I think we eventually did and will share more in the upcoming 2 posts!&lt;/p&gt;
&lt;h3&gt;Setup&lt;/h3&gt;
&lt;p&gt;One of the tools that I wanted to start playing around with was Amazon's EC2 servers because the trial version is free for the first year and I also wanted to learn the technology! However, the set up for the EC2 server was actually not as straight forward as I would have imagined. Particularly because I wanted to use PostgreSQL and directly connect to the server to pull data into my local machine to analyze with Python (Psycopg2 to the rescue).&lt;/p&gt;
&lt;h4&gt;Amazon Instance&lt;/h4&gt;
&lt;p&gt;Anyways, the first step was to create the EC2 instance:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Make sure that you have an AWS account and select a region that makes sense (in my case US West)&lt;/li&gt;
&lt;li&gt;Choose one of the configurations that you want (my teammates and I chose Ubuntu)&lt;/li&gt;
&lt;li&gt;Select a "free tier" instance ("t2.micro" for me!)&lt;/li&gt;
&lt;li&gt;Walk through the setup process by following the prompts&lt;/li&gt;
&lt;li&gt;Setup security groups&lt;ul&gt;
&lt;li&gt;This part was a bit confusing because PostgreSQL required another port to allow connections from my local machine's Python (will talk more about this later)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Choose to "Create a new key pair" and give it a name&lt;/li&gt;
&lt;li&gt;Download your .pem file.&lt;/li&gt;
&lt;li&gt;Move the file somewhere sensible like ~/.ssh/.&lt;/li&gt;
&lt;li&gt;Make the file read only with chmod 400 filename&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;At this point, the server is set up and can be accessed using:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ssh -i ~/.ssh/my_cool_machine.pem ubuntu@123.234.123.234
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Loading the tools and getting things ready&lt;/h4&gt;
&lt;p&gt;At this point we needed to get a couple packages onto the server so that we could get working. &lt;code&gt;apt-get&lt;/code&gt; is awesome for this when using Ubuntu.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;First of all we needed to be able to install Python related things:&lt;ul&gt;
&lt;li&gt;Use &lt;code&gt;pip&lt;/code&gt; to install Python things: &lt;code&gt;sudo apt-get install python-pip&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Other scipy things: &lt;code&gt;sudo apt-get install python-numpy python-scipy python-matplotlib ipython ipython-notebook python-pandas python-sympy python-nose&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;I like git and emacs :) : &lt;code&gt;sudo apt-get install git emacs&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Instead of signing in as 'Ubuntu' I added myself as a user:&lt;ul&gt;
&lt;li&gt;&lt;code&gt;sudo adduser [username]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Grant total access: &lt;code&gt;sudo visudo&lt;/code&gt; which will open a nano text file&lt;ul&gt;
&lt;li&gt;Add &lt;code&gt;[username] ALL=(ALL:ALL) ALL&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Setup public key by following this &lt;a href="http://docs.oracle.com/cd/E19253-01/816-4557/sshuser-33/index.html"&gt;link&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;Once the public key is generated copy it/ paste it onto remote server (copy/ paste probably isn't the best way... but it works!)&lt;ul&gt;
&lt;li&gt;Copy this &lt;code&gt;~/.ssh/id_rsa.pub&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Create proper files on remote machine &lt;code&gt;sudo mkdir /home/my_cool_username/.ssh/&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Paste the copied public key into the authorized keys file &lt;code&gt;sudo nano /home/my_cool_username/.ssh/authorized_keys&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;This should work now! &lt;code&gt;ssh my_cool_username@123.234.123.234&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Make it easier to login&lt;ul&gt;
&lt;li&gt;Edit ssh config file by adding&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Host my_cool_machine&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;User my_cool_username&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Edit &lt;code&gt;/etc/hosts&lt;/code&gt; file&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Host_ip my_cool_machine&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Finally we can now login using &lt;code&gt;ssh my_cool_machine&lt;/code&gt;&lt;ul&gt;
&lt;li&gt;And easily move things over to the remote machine using &lt;code&gt;scp cool_file.png my_cool_machine:~&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;PostgreSQL&lt;/h4&gt;
&lt;p&gt;In order to start using PostgreSQL I had to install PostgreSQL onto the remote machine and set up the access so that I could reach it from my local machine through Python.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Install PostgreSQL &lt;code&gt;sudo apt-get install postgresql postgresql-contrib&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Start and stop the PostgreSQL server using the following&lt;ul&gt;
&lt;li&gt;&lt;code&gt;sudo service postgresql status&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sudo service postgresql stop&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sudo service postgresql start&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Installing PostgreSQL created a &lt;code&gt;postgres&lt;/code&gt; user, but to add myself as a user I had to&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sudo -u postgres createuser --superuser my_user_name
sudo -u postgres psql
&lt;span class="c1"&gt;# now in psql...&lt;/span&gt;
&lt;span class="se"&gt;\p&lt;/span&gt;assword my_user_name
&lt;span class="c1"&gt;# exit out of psql with Ctrl+D&lt;/span&gt;
&lt;span class="c1"&gt;# Create a database for your user&lt;/span&gt;
sudo -u postgres createdb my_user_name
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Load the data&lt;/h3&gt;
&lt;p&gt;At this point the Postgres environment was set up but there was no data yet! So in order to add some data, I found it easiest to just directly import a &lt;code&gt;.csv&lt;/code&gt; file. However, I had to create the database first.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;CREATE&lt;/span&gt; &lt;span class="k"&gt;DATABASE&lt;/span&gt; &lt;span class="n"&gt;census&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Then switch to the newly created database &lt;code&gt;\c endor&lt;/code&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;CREATE&lt;/span&gt; &lt;span class="k"&gt;TABLE&lt;/span&gt; &lt;span class="n"&gt;census&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;id&lt;/span&gt; &lt;span class="nb"&gt;SERIAL&lt;/span&gt; &lt;span class="k"&gt;PRIMARY&lt;/span&gt; &lt;span class="k"&gt;KEY&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;age&lt;/span&gt; &lt;span class="nb"&gt;INT&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; 
    &lt;span class="n"&gt;workclass&lt;/span&gt; &lt;span class="nb"&gt;TEXT&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;fnlwgt&lt;/span&gt; &lt;span class="nb"&gt;INT&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;education&lt;/span&gt; &lt;span class="nb"&gt;TEXT&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;education_num&lt;/span&gt; &lt;span class="nb"&gt;INT&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;marital_status&lt;/span&gt; &lt;span class="nb"&gt;TEXT&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;occupation&lt;/span&gt; &lt;span class="nb"&gt;TEXT&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;relationship&lt;/span&gt; &lt;span class="nb"&gt;TEXT&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;race&lt;/span&gt; &lt;span class="nb"&gt;TEXT&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;sex&lt;/span&gt; &lt;span class="nb"&gt;TEXT&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;capital_gain&lt;/span&gt; &lt;span class="nb"&gt;INT&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;capital_loss&lt;/span&gt; &lt;span class="nb"&gt;INT&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;hours_per_week&lt;/span&gt; &lt;span class="nb"&gt;INT&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;native_country&lt;/span&gt; &lt;span class="nb"&gt;TEXT&lt;/span&gt;
&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now with the data table set up, I scp'ed the census data from my local machine to the remote machine and copied the data into database.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;COPY&lt;/span&gt; &lt;span class="n"&gt;census&lt;/span&gt; &lt;span class="k"&gt;FROM&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;/path/to/ewoks.csv&amp;#39;&lt;/span&gt; &lt;span class="k"&gt;DELIMITER&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;,&amp;#39;&lt;/span&gt; &lt;span class="n"&gt;CSV&lt;/span&gt; &lt;span class="n"&gt;HEADER&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Permissions&lt;/h3&gt;
&lt;p&gt;Now the Postgres database is set up and work can begin if I wanted to do all the work on my remote machine. However, it is much easier to work from my local machine so I had to change a few things in the Postgres configuration files and my Amazon AWS security group.&lt;/p&gt;
&lt;p&gt;Lets start with the AWS security group. Whenever I want to log into my remote machine from a different IP address I need to add a 'rule' that allows that new IP address (or I can set it to allow any IP but thats probably not the best). So under security group on my AWS I add a new SSH with my current IP address. But I also need to allow communication with the Postgres database. Luckily, AWS has a pretty smooth process for this. Once again I add a 'rule' but this time from the drop down I select 'PostgreSQL' and I can allow any IP address to access the databse or some specified ones.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Postgres AWS" src="/images/postgre_aws.png"&gt;&lt;/p&gt;
&lt;p&gt;In order to edit the Postgres configuration files, I need to go back into my remote machine. There are 2 files that I need to edit.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;sudo nano /etc/postgresql/9.3/main/postgresql.conf&lt;/code&gt;&lt;ul&gt;
&lt;li&gt;Under 'CONNECTIONS &amp;amp; AUTHENTIFICATION' change the 'listening_addresses' from 'localhost' to '*' this will allow all to access&lt;ul&gt;
&lt;li&gt;&lt;code&gt;listen_addresses = '*'&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sudo nano /etc/postgresql/9.3/main/pg_hba.conf&lt;/code&gt;&lt;ul&gt;
&lt;li&gt;Under 'IPv4 LOCAL CONNECTIONS' I change the allowed IPs&lt;ul&gt;
&lt;li&gt;&lt;code&gt;host all all 0.0.0.0/0 trust&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;FINALLY. At this point, I can actually access my remote database from my local machine. However, when using Python and psycopg2 to access the database there were a few more hurdles that I had to jump over :(.&lt;/p&gt;
&lt;h3&gt;Psycopg2 and Python&lt;/h3&gt;
&lt;p&gt;To directly connect with the database I had to install &lt;a href="http://initd.org/psycopg/"&gt;psycopg2&lt;/a&gt;, which is a Postgres adapter for Python. I did the following in order to get the set up working... (lots of steps and troubleshooting; took  me a long time to figure out the last step)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;brew&lt;/span&gt; &lt;span class="n"&gt;install&lt;/span&gt; &lt;span class="n"&gt;postgresql&lt;/span&gt;
&lt;span class="n"&gt;pip&lt;/span&gt; &lt;span class="n"&gt;install&lt;/span&gt; &lt;span class="n"&gt;psycopg2&lt;/span&gt;
&lt;span class="c1"&gt;# this next step I had to preform only because it kept throwing back an error saying that my&lt;/span&gt;
&lt;span class="c1"&gt;# AWS was not set up correctly but when I looked on stackoverflow it seemed like it was a psychopg2 bug&lt;/span&gt;
&lt;span class="n"&gt;brew&lt;/span&gt; &lt;span class="n"&gt;unlink&lt;/span&gt; &lt;span class="n"&gt;openssl&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;brew&lt;/span&gt; &lt;span class="n"&gt;link&lt;/span&gt; &lt;span class="n"&gt;openssl&lt;/span&gt; &lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="n"&gt;force&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Finishing up&lt;/h3&gt;
&lt;p&gt;Whew! That was exciting. I know...&lt;/p&gt;
&lt;p&gt;The process to get the server and database all set up may seem quite tedious, but many of the steps above are actually quite quick! Plus, it is good to have things all set up so that the analysis and visualization steps can happen smoothly! In the next post, I will discuss more of the details behind the analysis and also hopefully show a working version of the D3 dashboard!&lt;/p&gt;</content></entry><entry><title>What's Cooking?</title><link href="http://jeffwen.com/2015/12/19/whats_cooking" rel="alternate"></link><published>2015-12-19T11:01:00-05:00</published><updated>2015-12-19T11:01:00-05:00</updated><author><name>Jeff Wen</name></author><id>tag:jeffwen.com,2015-12-19:/2015/12/19/whats_cooking</id><summary type="html">&lt;p&gt;Blog post on my entry for the What's Cooking kaggle competition&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;December 19, 2015&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Update!&lt;/strong&gt;: The competition has now ended and I am in 87th place on the public leaderboard out of 1,388 teams/individuals. Top 6% for my first kaggle!&lt;/em&gt;&lt;/p&gt;
&lt;h3&gt;Getting Started&lt;/h3&gt;
&lt;p&gt;Ever since I have known about kaggle, I have always seen it as a place where accomplished data scientists and skilled veterans of the machine learning space go to stretch their legs and dominate. However, as I continued to look through the competitions and through the forums where aspiring data scientists were asking a wide variety of questions, I realized that my preconceptions were probably holding me back from learning a lot through practical application. It is definitely one thing to read about algorithms and techniques and quite another to actually implement. Anyways, with that in mind, I figured I would jump in and get started.&lt;/p&gt;
&lt;p&gt;Before I got started on the whats cooking competition, I had read a lot about the models that I was planning to use and had also read code from past competitions; however, jumping into the competition helped me actually get my hands dirty with models that I had only used a couple times before. More importantly, having to actually write the code myself, I got to practice being (or at least trying to be) systematic and comprehensive in my modeling decisions. Ultimately, I think the competition pushed me to not only read/learn, but also practice and be comfortable with an iterative problem solving process. &lt;/p&gt;
&lt;h3&gt;What's Cooking&lt;/h3&gt;
&lt;p&gt;The &lt;a href="https://www.kaggle.com/c/whats-cooking"&gt;competition&lt;/a&gt; as mentioned before was hosted on the kaggle platform. While there were many competitions to choose from, I decided to do this cooking challenge because I was interested in learning natural language processing, which I felt like I hadn't really been able to practice. Although the competition did not require much real language processing, it was still appealing and interesting to think about how different ingredients could be used to predict the cuisine. &lt;em&gt;(Most of the code for this competition is on my &lt;a href="https://github.com/jeffwen/Kaggle/blob/master/Whats%20Cooking/cooking.py"&gt;github&lt;/a&gt;. There are a few cleanup functions and exploratory scripts that I unfortunately have on another computer)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;For me, the largest reason for joining this competition was to learn. More than wanting to place well on the leaderboard, I wanted to understand what I was doing with the algorithms I was using and I wanted to make educated decisions about my modeling choices. &lt;em&gt;Then&lt;/em&gt;, if I happened to do well that would be an additional blessing. As a result, I spent lots of time learning by reading and tinkering. Instead of copying code from forums, I wanted to implement things myself, which took lots of time...lots of trial...and lots of error...&lt;/p&gt;
&lt;h4&gt;&lt;em&gt;Initial decisions&lt;/em&gt;&lt;/h4&gt;
&lt;p&gt;To start with, my plan was to (I eventually only had time to do up to number 7, but ended up with a satisfying score):&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Start by exploring the data and summarizing the data to understand how the classes look, how the ingredients are distributed across cuisines, and start brainstorming which models I wanted to use&lt;/li&gt;
&lt;li&gt;Process the data so that I could use it as input into the model that I had decided to move forward with&lt;/li&gt;
&lt;li&gt;Build a simple baseline model upon which I could further improve&lt;ul&gt;
&lt;li&gt;Logistic regression was good for this problem because its a simple model compared to the other models that I could have started with...&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;See if I need to change the way I was extracting features/ implement new feature extraction techniques to get more information from the data&lt;/li&gt;
&lt;li&gt;Fit the simple model again and tune the parameters&lt;/li&gt;
&lt;li&gt;Start experimenting with and tuning more complex models that make sense given the problem&lt;ul&gt;
&lt;li&gt;One thing that I am trying to learn more about is when to use a model given the input data because it is easy to throw a million models at a problem and see what works, but it takes experience and skill to figure out what model would work best given the nature of the data (though in averaging I use many random models because the point of the weighted averaging, as I know it, is to try to use weak learners and/or uncorrelated submission files to create a stronger learner/submission)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Think about simple averaging by coming up with a weighted average of my submissions to see if the model performs better&lt;/li&gt;
&lt;li&gt;Enter into the real black box stuff where I stack and blend models to build crazy ensembles&lt;ul&gt;
&lt;li&gt;Some really &lt;a href="http://mlwave.com/kaggle-ensembling-guide/"&gt;legit stuff&lt;/a&gt;...&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Anyways, the data comes in .json files, which python makes pretty easy to parse with the json package. As an example, the first entry in the data file looks like this&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="sa"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;cuisine&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="sa"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;greek&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
 &lt;span class="sa"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;id&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;10259&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
 &lt;span class="sa"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;ingredients&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="sa"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;romaine lettuce&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="sa"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;black olives&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="sa"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;grape tomatoes&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="sa"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;garlic&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="sa"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;pepper&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="sa"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;purple onion&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="sa"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;seasoning&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="sa"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;garbanzo beans&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="sa"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;feta cheese crumbles&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;em&gt;Note: I did a few things to look through the data and summarize/ explore the data before I got started. Namely, I looked at the counts of the each cuisine to see if there were any biased classes, which there were, but when I set custom weighting for the classes the models didn't change much so I decided I would return to this later. Furthermore, I looked to format the data correctly because things like the registered trademark signs caused encoding problems in the input data.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Given the way the data was formatted, I wanted to separate out the cuisine as the y-variable and have the x-variables be the ingredients. This is where the initial decisions had to be made.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How was I going to do the initial processing to set up my training data to extract features from the data?&lt;ul&gt;
&lt;li&gt;I could use a simple &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer"&gt;count vectorizer&lt;/a&gt;, which converts the words in a document (or in this case recipe) into counts or...&lt;/li&gt;
&lt;li&gt;binary encoding, which (if I implemented this correctly means 0 if the ingredient is not in the recipe and 1 if it is) or...&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer"&gt;tfidf (term frequency inverse document frequency) vectorizer&lt;/a&gt; to not only count but also return the normalized count based on how many times an ingredient appears in all the recipes&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Once the format of the training data was decided, how was I going to implement the vectorizer or binary encoder?&lt;ul&gt;
&lt;li&gt;This was a question I found myself asking because the standard vectorizer and encoder in sci-kit learn were slightly different than what I wanted to do.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I initially decided to write my own binary encoder because I thought that using a count vectorizer, though simple, would be throwing away information from the data. More specifically, I figured that, using the sample above, if my ingredients were ['romaine lettuce', 'black olives', 'grape tomatoes'...] and I used the stock count vectorizer then I would have the following as features ['romaine', 'lettuce', 'black', 'olives'...]. To me this was a problem because 'black olives' as one feature contained more information than 'black', 'olives' as two features.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I had decided from the beginning that I wanted to start with a bag of ingredients and not a bag of words.&lt;/li&gt;
&lt;li&gt;Though I knew that I could customize the analyzer function in the sci-kit learn package, I wanted to start with a baseline model before moving onto more complex models.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;binary_encoding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;list_of_ingredients&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;input_data&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;    list_of_ingredients: a list of ingredients that has been deduplicated and represents the features (column titles of the matrix)&lt;/span&gt;
&lt;span class="sd"&gt;    input_data: the data to be converted into a feature matrix&lt;/span&gt;
&lt;span class="sd"&gt;    returns a sparse feature matrix&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span class="n"&gt;pbar&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ProgressBar&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="c1"&gt;# progress bar to help me figure how much has been completed&lt;/span&gt;
    &lt;span class="n"&gt;aList&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;recipe&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;pbar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_data&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
        &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fromkeys&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;list_of_ingredients&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# creates a new dictionary with keys from list of ingredients with the initial value set to 0&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;word&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;recipe&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;word&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;list_of_ingredients&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="c1"&gt;# if the ingredient is in the list of ingredients, then change value to 1&lt;/span&gt;
        &lt;span class="n"&gt;aList&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
    &lt;span class="n"&gt;sparse_matrix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;scipy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sparse&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;csr_matrix&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;aList&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# create a sparse matrix&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;sparse_matrix&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;After implementing the function above on the data, and running a simply logistic regression model on the data, I received ~0.77 on the leaderboard. Though it wasn't great it was my first submission and I was quite happy. Even through this initial phase of feature extracting, I realized that a lot of the time I spent on this competition would be on feature engineering and processing.&lt;/p&gt;
&lt;h4&gt;&lt;em&gt;Moving forward&lt;/em&gt;&lt;/h4&gt;
&lt;p&gt;With the ~0.77 in the books, I decided it was time to start tuning and adding complexity.&lt;/p&gt;
&lt;p&gt;The first thing that I did was write a preprocessing function to help me clean the input data. I decided that I wanted to stem the words so that 'olives' would become 'olive' because the plural shouldn't lead to two different ingredients. Then I also stripped the words of punctuation and other non-alphabetic characters. After this initial step, I decided it would be good to sort the words in the ingredient list so that ingredients like 'feta cheese crumbles' and 'crumble feta cheese' would be one feature and not multiple after the word stemming and ingredient sorting (may or may not have been necessary, but I figured given the way the data was structured it wouldn't hurt).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;preprocess&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_data&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;new_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="n"&gt;pbar&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ProgressBar&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="c1"&gt;# Progress bar to ease the waiting&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;recipe&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;pbar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_data&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;new_recipe&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;ingredient&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;recipe&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;new_ingredient&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;word&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;ingredient&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
                &lt;span class="n"&gt;word&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sub&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;[^a-zA-Z -]+&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# only keeping the letters, spaces, and hyphens&lt;/span&gt;
                &lt;span class="n"&gt;new_ingredient&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;wn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;morphy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lower&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;,.!:?;&amp;#39; &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;,.!:?;&amp;#39; &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="c1"&gt;# strip, stem, and append the word&lt;/span&gt;
            &lt;span class="n"&gt;new_recipe&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;new_ingredient&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="n"&gt;new_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;new_recipe&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;new_data&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sort_ingredient&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_data&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;new_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="n"&gt;pbar&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ProgressBar&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;recipe&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;pbar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_data&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;new_recipe&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;ingredient&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;recipe&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;sorted_ingredient&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;sorted&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ingredient&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
            &lt;span class="n"&gt;new_recipe&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sorted_ingredient&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;new_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;new_recipe&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;new_data&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;After this preprocessing step, I move towards using &lt;a href="https://en.wikipedia.org/wiki/Tfidf"&gt;tf-idf&lt;/a&gt; because I figured having an indication of the frequency of a certain ingredient would provide additional information as opposed to simple binary encoding. So I used the slightly modified tf-idf vectorizer from sci-kit learn. Similar to the count vectorizer, the default analyzer seemed to split up the words of an ingredient into two words because of the way that my data was being passed in. To solve this, I wrote my own analyzer function that basically parsed the ingredient list and returned the entire ingredient (with all the words) as one feature &lt;em&gt;(think: ['romaine lettuce', 'black olives'] vs. ['romaine', 'lettuce', 'black', 'olives'])&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Ultimately, combining the preprocessing with the tf-idf vectorizing and a logistic regession model gave me ~0.778 on the leader board (an improvement!). So the next step was to figure out how to further tune the model. &lt;/p&gt;
&lt;h4&gt;&lt;em&gt;Model tuning&lt;/em&gt;&lt;/h4&gt;
&lt;p&gt;Modeling tuning seems to be an art in the machine learning world. This was where I felt out of my league because I did not have much experience. I had manually set complexity parameters before when pruning decision trees, but had not really experimented with changing the many parameters in the models I was using.&lt;/p&gt;
&lt;p&gt;So I spent lots of time reading through the forums to see what others were doing and saw that the top performers were using grid searches. I eventually ended up using a grid search to create my strongest performing single model, but initially when I was experimenting I used cross validation to manually see how changing the regularization parameter would affect my CV scores and found that with C=5 my model was scoring around ~0.782 with the CV that I was performing on my own system. When I uploaded the submission after predicting the new cuisines I got ~0.7857 so again there was an improvement (though it was not great that my own CV was returning scores that were slightly off. I probably will spend some more time figuring out how to model a more precise evaluation metric next time.)&lt;/p&gt;
&lt;p&gt;&lt;img alt="logreg tfidf score" src="/images/logreg_1.png"&gt;&lt;/p&gt;
&lt;p&gt;At this point in the competition there were around ~1000 or so competitors so I was in the top 50%, which was exciting considering just a few weeks ago I didn't even think I could create any worthwhile models.&lt;/p&gt;
&lt;h4&gt;&lt;em&gt;Rethinking my initial assumptions&lt;/em&gt;&lt;/h4&gt;
&lt;p&gt;Initially, I had decided not to use the standard sci-kit learn vectorizer analyzers because I felt that it would take away information from my data. However, upon further consideration, I realized that it was worth a try because with &amp;gt; 5000 features not all the features would be important anyways so why create more features by creating a bag of ingredients if a bag of words may actually help to reduce complexity a little bit.&lt;/p&gt;
&lt;p&gt;With this in mind, I rewrote my preprocessing function and started using the tf-idf vectorizer from sci-kit learn with a few parameters modified.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;preprocess_all_ingredients&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_data&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;new_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="n"&gt;pbar&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ProgressBar&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="c1"&gt;# Progress bar to ease the waiting&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;recipe&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;pbar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_data&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;new_recipe&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;ingredient&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;recipe&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;new_ingredient&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;word&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;ingredient&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
                &lt;span class="c1"&gt;# using a word lemmatizer, which is related to stemming, but takes into account context also (in this case other ingredients in the recipe)&lt;/span&gt;
                &lt;span class="n"&gt;word&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sub&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;[^a-zA-Z]&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="n"&gt;new_ingredient&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;WordNetLemmatizer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lemmatize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lower&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;,.!:?;&amp;#39; &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;,.!:?;&amp;#39; &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="n"&gt;new_recipe&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;new_ingredient&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="n"&gt;new_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;new_recipe&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;new_data&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The new function is similar to the previous preprocessing function but uses a lemmatizer instead of a stemmer (&lt;a href="http://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html"&gt;read more here&lt;/a&gt;), which basically takes context into account (though lemmatization may not have much use in this case given the ingredients are separate words in a string and not a cohesive sentence).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;tfidf_vectorizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;TfidfVectorizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;stop_words&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;english&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;ngram_range&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;analyzer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;word&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_df&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.56&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;token_pattern&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="sa"&gt;r&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;\w+&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;With the new preprocessing and vectorizer in place, I used a logistic regression again and got ~0.787. So there was again an improvement but at this point, I figured maybe I had come close to the extent of progress I would make with the logistic regression model. I did run a grid search over the regularization parameter and received a score of ~0.788. However, I was ready to move onto other models and thought that I would perhaps come back to logistic regressions if I were to do stacking and blending of models later on.&lt;/p&gt;
&lt;h4&gt;&lt;em&gt;Big improvements&lt;/em&gt;&lt;/h4&gt;
&lt;p&gt;The next steps in the modeling process led to huge improvements and at one point even got me to the top 3% of the leaderboard. &lt;/p&gt;
&lt;p&gt;At this time the top performers seemed to be using neural nets and xgb models (extreme gradient boosted trees). I tried xgb but my results were not great (the best xgb gave me ~0.78). &lt;/p&gt;
&lt;p&gt;With the basic preprocessing functions set, I felt like I should spend more time on reading and understanding how to tune my models. I had used support vector machines (SVM) before, but I was unsure if the model would handle the data given that my computer is old and SVMs are not known to perform particularly quickly if the dataset is too large. Given that I had thousand of features, I was afraid of the run time of using SVMs, but in the end after reading a couple resources (including &lt;a href="http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf"&gt;this one&lt;/a&gt;) I decided I would give it a try and just change to something else if necessary. Furthermore, SVMs have fewer hyperparameters to tune (so it would be easier to set up a grid search compared to say xgb or neural nets).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;param_grid&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;C&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;kernel&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;linear&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]},{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;C&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;gamma&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.001&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;kernel&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;rbf&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]}]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;I tried both linear and non-linear kernels and a variety of regularization parameters (I tried to read a lot on how to choose the parameters to place in the grid search). After fitting the model, which took at least 8 hours, I took a look at the parameter grid, which also shows the CV scores from the 3-fold CV that was performed in the grid search and noticed that the scores were pretty varied, but some scores were &amp;gt; 0.80. Of course I was excited, but not ecstatic because I knew that my CV was not exactly in line with the Kaggle leaderboard score calculation. When I submitted, I was surpised to see that my submission received a score on the leaderboard of ~0.81044 (&lt;strong&gt;HUGE improvement for me!&lt;/strong&gt;).&lt;/p&gt;
&lt;p&gt;&lt;img alt="SVM improvement" src="/images/svm_2.png"&gt;&lt;/p&gt;
&lt;p&gt;At this point there were ~1200 teams in the competition so my score put me somewhere in the top 3%! I was originally okay with the top 50%...but no complaints!&lt;/p&gt;
&lt;h4&gt;&lt;em&gt;Finishing touches&lt;/em&gt;&lt;/h4&gt;
&lt;p&gt;With only a couple days or so left in the competition, I didn't have much time to run additional stacked models or try tuning neural networks. However, I knew that I could squeeze a little more out of my submissions and so I spent some time researching stacking and blending of models. I had read that many recent winners had performed so well because they had implemented some form of stacking and/or blending (the netflix prize was won by an ensemble of &lt;a href="http://data-informed.com/in-awarding-prize-for-analytics-netflix-failed-to-predict-it-wouldnt-be-used/"&gt;800 different models&lt;/a&gt;). Of course, basic understanding of what each model is doing is very important and was my main goal, but at this point I felt like I could do little more.&lt;/p&gt;
&lt;p&gt;I basically ended up doing a weighted average of my top submission files (which is not stacking or blending but a precursor step). In particular, I gave my SVM 3x weight, logreg 1x, linear SVC 1x, another logreg 1x, and extra trees classifier 1x and ended up with a score of ~0.81225, which at the time placed me at 40th out of ~1300 teams.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Ensemble score" src="/images/ensemble_1.png"&gt;&lt;/p&gt;
&lt;h3&gt;Lots to learn&lt;/h3&gt;
&lt;p&gt;It has been such a journey starting with importing the .json files to getting (at the time) 40th on the leaderboard. Even though with a few more hours to go in the competition I figure I may drop a few places, I feel like I have gained at least a taste of what it feels like to enter and apply the things that I have learned.&lt;/p&gt;
&lt;p&gt;In entering this competition I got a chance to apply a lot of things that I had read about. Furthermore, I read &lt;strong&gt;EVEN MORE&lt;/strong&gt; things that I didn't get to try out but hope to in the future. I have listed a few take-aways and things to try in the future.&lt;/p&gt;
&lt;h4&gt;&lt;em&gt;Next steps&lt;/em&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Use Levenstein distance to map similar words together (I actually have a function in my &lt;a href="https://github.com/jeffwen/Kaggle/blob/master/Whats%20Cooking/cooking.py"&gt;cooking code&lt;/a&gt; that I was going to use, but I never got around to implementing it...)&lt;/li&gt;
&lt;li&gt;Use Bayesian Optimization or Random Search to optimize parameter settings&lt;/li&gt;
&lt;li&gt;Stack and blend different models&lt;/li&gt;
&lt;li&gt;Experiment with other models (i.e. neural networks, xgb (with good parameter tuning))&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;em&gt;Take-aways&lt;/em&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Parameter tuning is very important and though grid search worked out well this time, its really computationally intensive and in another setting I might not have the luxury to perform this type of exhaustive search. I can try random search, which may be better but a very interesting thing that I read about were two packages called: Spearmint and Hyperopt, which use bayesian optimization to perform the parameter search (read this awesome &lt;a href="https://jmhessel.github.io/Bayesian-Optimization/"&gt;blog post&lt;/a&gt; or &lt;a href="http://fastml.com/optimizing-hyperparams-with-hyperopt/"&gt;this one&lt;/a&gt; to learn more about parameter tuning and bayesian optimization)&lt;/li&gt;
&lt;li&gt;Fitting models is easy, but figuring which model to use and when is difficult&lt;/li&gt;
&lt;li&gt;The need for a representative cross validation evaluation metric is very important if you want to know how your model is performing. I think this is why some teams spend quite a bit of time writing their own evaluation function that matches the competition's evaluation function because if they tell you how you will be judged why not use it!&lt;/li&gt;
&lt;li&gt;Simple models can actually perform really well when tuned properly. Given that simple models are MUCH faster to train it may be better to use a simple model if the application does not require absolute accuracy (this really depends on the problem at hand; healthcare is not an area where we want to use less accurate models just to speed up the modeling process)&lt;/li&gt;
&lt;li&gt;The entire problem solving approach has to iterative.&lt;/li&gt;
&lt;li&gt;Feature engineering and preprocessing take lots of time but are very important! I think this is where industry domain becomes handy because if I knew more about cooking I might have been able to create some more informative features&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;The End!&lt;/h3&gt;
&lt;p&gt;There are a lot of things that I have left out of this post in regards to my process and thinking but if there are any questions feel free to email me! I hope that this is just a start of my continued learning through kaggle and similar platforms. &lt;/p&gt;</content></entry></feed>